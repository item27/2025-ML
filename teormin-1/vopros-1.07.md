# Вопрос №1.07

> Как и зачем дискретизировать, бинаризовать, нормализовывать и взвешивать признаки?

**Ответ:**

В машинном обучении **преобразование признаков (feature preprocessing)** помогает упростить данные, сделать их пригодными для обучения и повысить качество модели.

**Дискретизация (Discretization)**

**Что это:**

Преобразование **непрерывных числовых признаков** в **дискретные интервалы** (категории).

**Пример:**

Температура (°C):

`[1, 5, 9, 13, 17, 21, 25, 29]`

→ Разделим на 3 интервала:

`[низкая=0, средняя=1, высокая=2]`

→ `[0, 0, 0, 1, 1, 1, 2, 2]`

**Зачем:**

* Уменьшает влияние шума и выбросов;
* Упрощает структуру данных;
* Подходит для алгоритмов, работающих с категориальными признаками (например, деревья решений);
* Может помочь уменьшить переобучение.

**Бинаризация (Binarization)**

**Что это:**

Преобразование **категориальных признаков** в **набор бинарных (0/1)**.

Самый известный способ - **One-Hot Encoding**.

**Пример:**

Цвет = `[красный, синий, зелёный]`

→ One-hot:

красный = (1,0,0)

синий = (0,1,0)

зелёный = (0,0,1)

**Зачем:**

* Делает категориальные признаки понятными для моделей, работающих с числами (линейная регрессия, SVM, нейросети);
* Избегает ложного "порядка" между категориями (в отличие от label encoding).

Другие варианты кодирования:

* **Label Encoding** - просто заменить категории числами (подходит для деревьев);
* **Binary Encoding** - компактная версия One-Hot для большого числа категорий.

**Нормализация и стандартизация**

**Нормализация (Normalization)** - приведение признаков к **одному масштабу**, например в диапазон `[0, 1]`:

$$
x' = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

**Стандартизация (Standardization)** - приведение к **среднему 0 и стандартному отклонению 1**:

$$
x' = \frac{x - \mu}{\sigma}
$$

**Зачем:**

* Чтобы признаки с разными масштабами **не искажали обучение** (особенно в градиентных методах и KNN);
* Ускоряет и стабилизирует обучение (градиентный спуск сходится быстрее);
* Делает веса интерпретируемыми.

**Пример:**

Если один признак изменяется от 0 до 1, а другой от 0 до 1 000 000 - без нормализации модель будет считать второй важнее просто из-за масштаба.

**Взвешивание признаков (Feature weighting)**

**Что это:**

Назначение признакам **разной важности (веса)** при обучении или при вычислении расстояний между объектами.

**Зачем:**

* Учесть предметную область (некоторые признаки объективно важнее);
* Снизить влияние шумовых признаков;
* Улучшить качество в алгоритмах, где используется расстояние (**kNN**, **K-Means**) или линейная комбинация признаков.

**Примеры:**

* Умножить важный признак на больший коэффициент:

`x_weighted = x * w`

* Использовать **автоматическое взвешивание** через регуляризацию, feature importance (например, в Random Forest или XGBoost).

**Краткое сравнение:**

| Операция                          | Что делает                                           | Пример                                 | Зачем                             |
| --------------------------------- | ---------------------------------------------------- | -------------------------------------- | --------------------------------- |
| **Дискретизация**                 | Делит непрерывные значения на интервалы              | Возраст → \[0: 0–20, 1: 21–40, 2: 41+] | Упрощение, устойчивость           |
| **Бинаризация**                   | Преобразует категориальные признаки в набор бинарных | Цвет → (is\_red, is\_blue...)          | Для моделей, работающих с числами |
| **Нормализация / стандартизация** | Приводит к единому масштабу                          | \[0, 1000] → \[0, 1]                   | Ускорение и стабильность обучения |
| **Взвешивание**                   | Назначает признакам разную важность                  | Вес возраста ↑, вес пола ↓             | Учёт значимости признаков         |
