# Вопрос №1.20

### Ансамблирование алгоритмов (Ensemble Learning)

Ансамблирование — это подход, при котором объединяются несколько моделей, чтобы получить более точный и устойчивый результат, чем каждая модель по отдельности.

Идея:

> “Группа слабых моделей может работать лучше одной сильной”.

***

### Основные типы ансамблей

| Тип                | Принцип                                                                                           | Пример                               |
| ------------------ | ------------------------------------------------------------------------------------------------- | ------------------------------------ |
| Бэггинг (Bagging)  | Обучаем модели независимо на разных подвыборках данных и усредняем их предсказания                | Random Forest                        |
| Бустинг (Boosting) | Обучаем модели последовательно, каждая исправляет ошибки предыдущей                               | AdaBoost, Gradient Boosting, XGBoost |
| Стекинг (Stacking) | Обучаем несколько разных моделей и потом метамодель, которая учится комбинировать их предсказания | StackEnsemble, Blending              |

***

### Бэггинг (коротко)

* Модели обучаются параллельно на случайных подвыборках данных (с возвращением).
* Предсказания усредняются (для регрессии) или голосуются (для классификации).
* Снижает дисперсию и риск переобучения.

Пример: Random Forest — ансамбль деревьев решений, где каждое дерево обучается на случайном подмножестве признаков и объектов.

***

### Бустинг (Boosting)

Бустинг — последовательное ансамблирование слабых моделей (обычно деревьев решений).\
Каждая следующая модель учится исправлять ошибки предыдущей.

Идея:

> Учимся “на ошибках” — новые модели фокусируются на объектах, где предыдущие ошибались.

***

### Математическая интуиция бустинга

1. Изначально модель делает простое предсказание (например, среднее по данным).
2. Вычисляем ошибку (остатки, residuals).
3. Обучаем следующую модель предсказывать эти ошибки.
4. Итоговое предсказание = сумма (или взвешенная сумма) всех базовых моделей.

$$
\hat{y} = \sum_{m=1}^M \alpha_m h_m(x)
$$

где

* $$h_m(x)$$ — слабая модель (например, дерево),
* $$\alpha_m$$ — вес модели,
* $$M$$ — количество итераций (глубина бустинга).

***

### Примеры бустингов

| Алгоритм                    | Ключевая идея                                                                                                      |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| AdaBoost                    | Каждая следующая модель получает больший вес для объектов, где были ошибки. Ошибочные примеры становятся “важнее”. |
| Gradient Boosting (GBM)     | Модели учатся предсказывать градиенты функции потерь. Более гибкий и устойчивый способ.                            |
| XGBoost, LightGBM, CatBoost | Современные оптимизированные реализации GBM. Ускоряют обучение и уменьшают переобучение.                           |

***

### Преимущества и недостатки бустинга

Плюсы:

* Очень высокая точность;
* Умеет работать с нелинейными зависимостями;
* Хорошо обрабатывает несбалансированные данные.

Минусы:

* Медленнее в обучении (последовательный процесс);
* Склонен к переобучению без регуляризации;
* Много гиперпараметров (learning rate, depth, n\_estimators и т.д.).

***

### Пример интуиции

Представь, что у тебя 10 студентов, каждый немного ошибается на тесте.\
Если собрать их ответы вместе (усреднить или проголосовать),\
результат будет точнее, чем у любого отдельного студента.

***

Итог:

> Ансамблирование — это “коллективный разум” моделей.\
> Бэггинг уменьшает разброс (variance),\
> бустинг уменьшает смещение (bias),\
> а вместе они делают модель сильнее и устойчивее.
