# Вопрос №1.16

> Градиентный спуск. Как оптимизирует параметры моделей, что такое пространство ошибок, чем отличаются (мини-)батчевый и стохастический

**Ответ:**

**Идея:** минимизируем функцию потерь $$L(w)$$ пошагово.

Обновление весов:

$$
w \leftarrow w - \eta ,\nabla_w L(w)
$$

где $$\eta$$ — скорость обучения, $$\nabla_w L$$ — градиент.

**Пространство ошибок:** гиперповерхность значений $$L(w)$$ над пространством параметров. Каждая точка — набор весов, высота — значение потерь.

**Варианты:**

* **Batch GD:** градиент по всей выборке. Стабильно, медленно.
* **Stochastic GD (SGD):** по одному объекту. Быстро, шумно, помогает выходить из локальных минимумов.
* **Mini-batch GD:** по мини-пакету (m) объектов. Компромисс скорость/стабильность. Стандарт де-факто.

**Практика:** нормализация признаков, убывающий $$\eta$$, моменты/адаптивные методы (Momentum, Adam), early stopping.
