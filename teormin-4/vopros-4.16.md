# Вопрос №4.16

> Beam search. Зачем нужен, как работает

## Зачем нужен

При генерации текста модель даёт распределение вероятностей для следующего токена. Greedy-подход берёт самый вероятный токен и может застрять в локальном максимуме. Beam search — компромисс между качеством и скоростью: он исследует несколько лучших гипотез одновременно, чтобы найти более вероятную полную последовательность без полного перебора.

## Как работает

1. **Параметр:** ширина луча (`beam width`, `k`) — сколько гипотез держим одновременно.
2. **Инициализация:** начинаем с пустой последовательности и вероятности 1 (или лог-вероятности 0).
3. **Расширение:** на каждом шаге для каждой текущей гипотезы генерируем все возможные следующие токены (или top-N) и обновляем суммарную лог-вероятность:  
   `score = log P(seq) + log P(next_token | seq)`.
4. **Обрезка:** среди всех расширенных гипотез оставляем `k` с наибольшими лог-вероятностями.
5. **Стоп:** когда гипотеза достигает токена `<EOS>` или максимальной длины, помещаем её в список завершённых. Продолжаем, пока не соберём нужное количество завершённых последовательностей или достигнем лимита шагов.
6. **Выбор ответа:** обычно берут завершённую гипотезу с максимальной (нормированной) лог-вероятностью. Иногда нормируют на длину, чтобы не предпочитать короткие последовательности (length penalty).

## Интуиция

Beam search ходит по дереву возможных текстов, но вместо полного перебора запоминает только `k` самых перспективных веток. При достаточной ширине ищет намного лучше, чем greedy, но дороже вычислительно (примерно в `k` раз). Ширина подбирается в зависимости от задачи: для перевода часто 5–10, для генерации кода/текста — 1–5.
