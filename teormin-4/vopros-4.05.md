# Вопрос №4.05

> Что такое корпус, что такое словарь, как размер словаря влияет на алгоритмы. Как уменьшить размер словаря, на что это повлияет

## Корпус и словарь

* **Корпус** — набор текстов (документов, предложений), на котором обучаем модели или считаем статистики. Важно, чтобы он отражал ту задачу/домен, для которого подготавливается система.
* **Словарь (vocabulary)** — множество уникальных токенов (слов, подслов, символов), которым присвоены индексы. Все методы кодирования полагаются на этот список.

## Как размер словаря влияет на алгоритмы

* **Память и вычисления.** У линейных моделей и нейросетей есть embedding-матрицы размера `|V| × d`. Чем больше `|V|`, тем больше параметров и памяти. Для one-hot/BoW матрица признаков становится шире и разреженнее.
* **Качество статистик.** Если словарь огромный, многие слова встречаются считанные разы → оценить их частоты сложно, веса становятся шумными, модель переобучается.
* **Скорость обучения.** Больше токенов — больше данных для обновления, медленнее итерации, особенно в softmax (нужно вычислять логиты для каждого слова).
* **OOV (out-of-vocabulary).** Если словарь маленький, повышается доля неизвестных слов, которые заменяются `<UNK>` или кодируются длинными последовательностями подслов. Это ухудшает понимание редких терминов.

Итак, нужно искать баланс: слишком большой словарь = дорогой и шумный, слишком малый = теряем информацию.

## Как уменьшить словарь и последствия

Способы:

1. **Фильтрация по частоте.** Удаляем редкие токены (например, встречающиеся < 5 раз). Влияет на редкие слова: они либо теряются, либо попадают в `<UNK>`.
2. **Удаление стоп-слов.** Убираем часто встречающиеся, но малоинформативные токены. Снижает размер словаря и шум, но может потеряться грамматика.
3. **Лемматизация/стемминг.** Сводим формы слов к базовой форме, объединяя вариации. Уменьшает словарь, но теряет падеж/время/число.
4. **n-gram pruning.** При использовании n-gram оставляем только самые частые сочетания.
5. **Субсловные методы (BPE, WordPiece).** Вместо полного словаря слов кодируем подсловами; итоговый словарь меньше и гибче.
6. **Символьное кодирование.** Обычно используется, если нужно минимизировать словарь до алфавита (но длина последовательностей растёт).

На что это влияет:

* **Качество представлений.** Удаляя редкие слова или склеивая формы, потенциально теряем смысловые различия.
* **Размер embedding-матриц и скорость.** Меньшее `|V|` снижает память и ускоряет обучение/инференс.
* **Обработка редких токенов.** Более компактный словарь требует спец-техник (UNK, BPE), чтобы всё равно кодировать редкие/новые слова.
* **Интерпретируемость.** После агрессивных преобразований (стемминг, BPE) токены менее удобны человеку, но модели их понимают.

При подготовке данных подход выбирают исходя из задачи: в классификации можно смело сокращать словарь, в генерации текста важно сохранить разнообразие и использовать субсловные токены, чтобы не терять редкие слова.
