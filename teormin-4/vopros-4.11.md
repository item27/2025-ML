# Вопрос №4.11

> Attention. Зачем нужен, геометрический смысл

## Зачем нужен attention

Attention-слой позволяет модели динамически выделять важные элементы входной последовательности при формировании представления очередного токена. В отличие от чистых RNN, где информация жёстко «зашита» в скрытом состоянии, attention даёт прямой доступ ко всем элементам контекста.

Основные цели:

* **Фокусировка на релевантных частях.** При переводе слово в выходе может зависеть от далёкого слова во входе; attention помогает выбрать нужные источники.
* **Доступ к дальним зависимостям.** Не нужно хранить всю информацию в одном векторе; можно напрямую обратиться к нужному токену.
* **Интерпретируемость.** Веса внимания показывают, на какие слова модель опиралась, что облегчает анализ.

## Геометрический смысл

Вектор внимания — взвешенная сумма value-векторов, где веса получены из сходства query и key.

1. **Query (`q`)** описывает «что ищем» (например, текущий токен в decoder).
2. **Key (`k`)** — представление каждого кандидата в контексте (encoder outputs или те же токены в self-attention).
3. **Value (`v`)** — содержимое, которое будем смешивать.

Скалярное произведение `q · k` (часто масштабированное) измеряет, насколько они направлены в одну сторону. Если представить эмбеддинги в `d`-мерном пространстве, attention сравнивает углы между `q` и всеми `k`. После softmax получаем веса, которые указывают, в чью сторону «смотрит» query. Затем берём взвешенную сумму value-векторов, получая новый вектор, который лежит внутри выпуклой оболочки values.

Таким образом attention — это операция, которая переносит query в новое положение, комбинируя наиболее «со-направленные» value-векторы. Это и есть геометрический смысл: поиск близких направлений (по cosine/inner product) и построение новой точки как их линейной комбинации.
