# Вопрос №4.15

> Какие сейчас существуют LLM? Чем LLM отличается от простых языковых моделей? Сколько весов (= параметров) у GPT-3.5 и у GPT-4? Что означает размер контекста у LLM?

## Примеры современных\* LLM

* **OpenAI:** GPT-3, GPT-3.5 (text-davinci-003, gpt-3.5-turbo), GPT-4 (gpt-4, gpt-4-turbo).
* **Google:** PaLM 2, Gemini (Nano/Pro/Ultra).
* **Meta:** LLaMA 2, LLaMA 3 (разные размеры: 7B, 13B, 70B, 405B).
* **Anthropic:** Claude 2/3.
* **Mistral AI:** Mistral 7B, Mixtral 8x7B, 8x22B.
* **Другие:** Falcon, BLOOM, Baidu ERNIE, Huawei PanGu, Aleph Alpha Luminous и т.д.

LLM — общий термин: модели с сотнями миллионов до триллиона параметров, способные выполнять множество задач при минимальном обучении.

## Чем LLM отличается от «простых» языковых моделей

* **Масштаб:** LLM содержат сотни млрд параметров, обучены на гигантских корпусах. Классические LM (n-gram, небольшие RNN) имеют гораздо меньше параметров и ограничены в возможностях.
* **Архитектура:** современные LLM почти всегда трансформеры, поддерживающие attention. Простые LM могли быть n-gram или маленькими RNN.
* **Способности:** LLM умеют zero-shot/few-shot генерацию, выполнять инструкции, код, рассуждения. Простые LM предсказывают следующий токен, но без общего «понимания».
* **Обучение:** LLM часто дообучают с RLHF/инструкциями для улучшения поведения. Простые LM — чистый next-token prediction.

## Количество параметров

* **GPT-3.5:** официально не раскрыто. Считается, что gpt-3.5-turbo — это модифицированная GPT-3 (≈175B) или GPT-3.5 (~180B). Точных цифр OpenAI не приводит.
* **GPT-4:** также не опубликовано. Неофициальные оценки — 1–1.8 триллиона параметров (возможно, смесь экспертов). Известно только, что модель существенно больше GPT-3.5.

Главное — GPT-3.5 порядка сотен миллиардов параметров, GPT-4 — значительно больше.

## Размер контекста

Размер контекста (context window) — максимальное количество токенов, которое модель может обработать за один запрос. Например, GPT-3.5-turbo = 4К токенов, GPT-4-turbo = 128К токенов. Внутри трансформера это определяется числом позиций, на которых обучены позиционные эмбеддинги и структурой attention. Чем больше окно:

* тем длиннее документы или диалоги можно учитывать,
* тем больше потребление памяти/времени, т.к. attention имеет квадратичную сложность по длине,
* тем сложнее поддерживать стабильность обучения.

Размер контекста задаёт ограничение на «объём истории», который модель видит одновременно.
