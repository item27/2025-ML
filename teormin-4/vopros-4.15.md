# Вопрос №4.15

> Какие сейчас существуют LLM? Чем LLM отличается от простых языковых моделей? Сколько весов (= параметров) у GPT-3.5 и у GPT-4? Что означает размер контекста у LLM?

## Примеры современных LLM

* **OpenAI:** GPT-3, GPT-3.5 (text-davinci-003, gpt-3.5-turbo), GPT-4 (gpt-4, gpt-4-turbo).
* **Google:** PaLM 2, Gemini (Nano/Pro/Ultra).
* **Meta:** LLaMA 2 (7B, 13B, 70B), LLaMA 3 (8B, 70B; более крупные версии анонсированы, но публичных цифр нет).
* **Anthropic:** Claude 2/3.
* **Mistral AI:** Mistral 7B, Mixtral 8x7B, 8x22B.
* **Другие:** Falcon, BLOOM, Baidu ERNIE, Huawei PanGu, Aleph Alpha Luminous и т.д.

LLM — общий термин: модели с сотнями миллионов до триллиона параметров, способные выполнять множество задач при минимальном обучении.

## Чем LLM отличается от «простых» языковых моделей

* **Масштаб:** LLM содержат сотни млрд параметров, обучены на гигантских корпусах. Классические LM (n-gram, небольшие RNN) имеют гораздо меньше параметров и ограничены в возможностях.
* **Архитектура:** современные LLM почти всегда трансформеры, поддерживающие attention. Простые LM могли быть n-gram или маленькими RNN.
* **Способности:** LLM умеют zero-shot/few-shot генерацию, выполнять инструкции, код, рассуждения. Простые LM предсказывают следующий токен, но без общего «понимания».
* **Обучение:** LLM часто дообучают с RLHF/инструкциями для улучшения поведения. Простые LM — чистый next-token prediction.

## Количество параметров

* **GPT-3.5:** OpenAI не раскрывает точные цифры. По оценкам, это модификация GPT-3 порядка 170–180B параметров.
* **GPT-4:** число параметров не публиковалось. Независимые оценки — сотни миллиардов или смесь экспертов порядка триллиона; известно только, что модель заметно крупнее GPT-3.5.

Главное — GPT-3.5 порядка сотен миллиардов параметров, GPT-4 — значительно больше.

## Размер контекста

Размер контекста (context window) — максимальное количество токенов, которое модель может обработать за один запрос. Например, gpt-3.5-turbo = 4–16К токенов, gpt-4-turbo = 128К токенов. Внутри трансформера это определяется числом позиций, на которых обучены позиционные эмбеддинги и структурой attention. Чем больше окно:

* тем длиннее документы или диалоги можно учитывать,
* тем больше потребление памяти/времени, т.к. attention имеет квадратичную сложность по длине,
* тем сложнее поддерживать стабильность обучения.

Размер контекста задаёт ограничение на «объём истории», который модель видит одновременно.
