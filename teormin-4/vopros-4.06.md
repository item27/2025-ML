# Вопрос №4.06

> Что такое byte pair encoding и как он сокращает размер словаря до минимума, но все равно способен кодировать даже те слова, которые не видел при сборке словаря

## Что такое BPE

Byte Pair Encoding (BPE) — алгоритм построения словаря подслов. Исходно это метод компрессии, но в NLP его используют для токенизации. Он итеративно объединяет самые частые пары символов (или уже слитых подслов) в новые токены, создавая компактный словарь, который всё равно умеет собирать любые слова из более мелких кусочков.

## Как работает

1. **Старт:** разбиваем каждое слово на последовательность символов (часто с добавлением символа конца слова `</w>`). Начальный словарь = все уникальные символы.
2. **Подсчёт частот пар:** смотрим, какие соседние символы/подслова встречаются чаще всего в корпусе.
3. **Слияние:** выбираем самую частую пару и заменяем её на новый токен. Добавляем его в словарь.
4. **Повторяем** шаги 2–3, пока не достигнем желаемого размера словаря (например, 30K подслов).

В итоге получаем список «merge rules» (какие пары сливались) и итоговый словарь подслов.

## Почему сокращает словарь

* **Компрессия:** вместо множества форм целых слов мы храним базовые символы + популярные сочетания. Словарь растёт контролируемо (по числу merge-итераций), а не по количеству уникальных слов.
* **Разделение редких слов:** редкие слова не выделяются в словарь полностью, а остаются комбинацией более частых подслов, поэтому не занимают отдельные позиции.
* **Настройка размера:** можно остановиться на любом количестве merge-операций. Таким образом словарь минимально необходимый: набор часто встречающихся кусков + символы для остальных случаев.

## Как кодирует невиданные слова

Pечевые слова разбиваются согласно тем же merge rules. Если целого слова нет в словаре, алгоритм будет применять правила слияния столько, сколько сможет, а оставшиеся части распадутся на более мелкие подслова вплоть до отдельных символов. Поскольку исходный словарь всегда содержит базовый алфавит (символы), любое слово можно представить как последовательность этих символов.

Итог: BPE одновременно

* уменьшает словарь по сравнению с «всеми словами»,
* сохраняет способность кодировать OOV-слова,
* позволяет моделям работать на подсловных единицах, что улучшает генерацию и понимание редких/новых слов.
