# Вопрос №4.10

> Модель (тип моделей) sequence2sequence. Какие задачи может решать, из чего состоят encoder и decoder

## Что такое sequence-to-sequence (seq2seq)

Seq2seq — семейство моделей, которые преобразуют входную последовательность переменной длины в выходную последовательность (тоже переменной длины). Архитектура состоит из двух частей: encoder и decoder, которые обучаются end-to-end.

## Какие задачи решает

* **Машинный перевод:** вход — предложение на языке A, выход — на языке B.
* **Суммаризация текста:** делаем короткий пересказ длинного документа.
* **Диалоговые системы:** реагирует на вопрос/реплику.
* **Распознавание речи → текст:** вход — последовательность акустических признаков.
* **Captioning изображений/видео:** вход — фичи из CNN, выход — текстовое описание (формально это тоже seq2seq, где encoder — CNN).
* **Парсинг, генерация кода, грамматическая коррекция** — любые задачи «входная последовательность → выходная последовательность».

## Из чего состоят encoder и decoder

### Encoder

* Принимает последовательность входных токенов и преобразует их в промежуточное представление.
* Исторически это был RNN/LSTM/GRU, который проходил вход слева направо (иногда двунаправленно) и выдавал финальное скрытое состояние.
* В современных трансформерах encoder — набор multi-head self-attention + feed-forward блоков, которые создают последовательность контекстных векторов (по одному на токен).
* Результат: либо один вектор-контекст (старый вариант), либо целая последовательность скрытых состояний (для attention).

### Decoder

* Генерирует выход по одному токену, опираясь на:
  * ранее сгенерированные токены (через рекурсию или masked self-attention),
  * контекст encoder’а (через attention к его выходам).
* В классической RNN-архитектуре decoder — это RNN, которому на каждом шаге подают предыдущий выход и скрытое состояние. В трансформерах decoder тоже состоит из attention-блоков: masked self-attention + cross-attention к encoder outputs.
* Возвращает распределение по словарю (softmax) для следующего токена, а обновлённое скрытое состояние/attention передаётся на следующий шаг.

Таким образом seq2seq — это пара encoder/decoder с общей функцией потерь (кросс-энтропия по целевой последовательности). Конкретная реализация (RNN, LSTM, Transformer) может меняться, но структура «вход→encoder→контекст→decoder→выход» сохраняется.
