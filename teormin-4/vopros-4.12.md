# Вопрос №4.12

> Attention. Как получается вес (= важность) эмбеддинга слова по отношению к другому слову?

Веса внимания — это нормированные оценки сходства между query (текущий токен) и key (кандидат из контекста). Для стандартного scaled dot-product attention всё выглядит так:

1. Для каждого слова в последовательности вычисляем три проекции: `q_i = W^Q x_i`, `k_i = W^K x_i`, `v_i = W^V x_i`.
2. Чтобы узнать важность слова `j` для слова `i`, считаем скалярное произведение `score_{i,j} = q_i · k_j`. Чем больше, тем более «со-направлены» векторы (т. е. тем словесно ближе значения).
3. Масштабируем оценку `score_{i,j} / sqrt(d_k)` — чтобы стабилизировать градиенты при большой размерности.
4. Пропускаем все оценки `score_{i,*}` через softmax:  
   `α_{i,j} = softmax_j(score_{i,*}) = exp(score_{i,j}) / Σ_j exp(score_{i,j})`.
   Получаем веса внимания, которые суммируются до 1 и можно трактовать как вероятности.
5. Новый вектор для слова `i` вычисляем как взвешенную сумму value-векторов:  
   `z_i = Σ_j α_{i,j} v_j`. Таким образом сам эмбеддинг изменяется на комбинацию других токенов в зависимости от их веса.

То есть важность = нормированная мера близости query текущего слова к key остальных слов. Если используется multi-head attention, описанные шаги выполняются параллельно для нескольких наборов матриц (`W^Q_h`, `W^K_h`, `W^V_h`), а результаты конкатенируются.
