# Вопрос №4.04

> Модель tf-idf. Какого размера вектор возвращает, какие данные хранит, как обучается, в чем минусы по сравнению с RNN

## Что такое TF-IDF

TF-IDF (Term Frequency–Inverse Document Frequency) — статистика, которая оценивает важность слова для конкретного документа относительно всего корпуса. Итог — разреженный вектор признаков, который подсказывает, какие слова «характерные» для текста.

### Какого размера вектор

Размер = число элементов словаря `|V|`. Для каждого слова в словаре есть одна координата, поэтому при переформатировании корпуса в TF-IDF представление получаем матрицу `N_docs × |V|`.

### Какие данные хранит

На позиции `i` для документа `d` лежит значение `tf_{d,i} * idf_i`.

* `tf_{d,i}` — частота слова в документе. Может быть абсолютной, нормированной (делится на длину документа) или логарифмической.
* `idf_i = log( (N_docs + 1) / (df_i + 1) ) + 1`, где `df_i` — количество документов, в которых слово встретилось. Чем слово реже, тем больше `idf`.

Матрица TF-IDF хранит вес каждого слова в каждом документе; отдельно приходится хранить словарь (отображение токен → индекс) и вектор `idf`.

### Как обучается

«Обучение» = подсчёт статистик:

1. Собираем словарь на основе корпуса (возможно, после фильтрации stop-слов, лемматизации).
2. Проходим по каждому документу, считаем `tf` и `df`.
3. Рассчитываем `idf` по формуле.
4. Для каждого документа формируем вектор `tf * idf`.

Никаких обучаемых весов нет — это детерминированная обработка корпуса.

## Минусы по сравнению с RNN

* **Нет порядка слов.** TF-IDF рассматривает текст как мешок терминов; последовательные зависимости и грамматика игнорируются. RNN читает токены по порядку и может моделировать контекст.
* **Фиксированная размерность и разреженность.** Вектор растёт с размером словаря, в большинстве координат нули. RNN использует компактные эмбеддинги и скрытые состояния фиксированной длины независимо от словаря.
* **Отсутствие обобщения по синонимам.** TF-IDF считает разные слова независимыми. RNN (с обучаемыми эмбеддингами) может сближать синонимы и даже выводить смыслы из контекста.
* **Не моделирует длинные зависимости.** Вес слова зависит только от его частоты и глобальной редкости, а не от соседних слов. RNN может накапливать информацию через скрытое состояние.
* **Не адаптируется под задачу.** TF-IDF не оптимизируется под конкретный таргет (классификация, генерация). RNN обучается end-to-end и подстраивает представления под целевую функцию.

Поэтому TF-IDF остаётся простым baseline/признаковым преобразованием, а RNN и другие нейросетевые модели применяются, когда нужен контекст и обучение на задаче.
