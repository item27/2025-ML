# Вопрос №4.08

> LSTM ячейка: чем отличается от простой RNN, за что отвечают гейты, как борется со взрывом/затуханием градиентов

## Отличия LSTM от простой RNN

* Вводит отдельную память (`cell state` `c_t`) помимо скрытого состояния `h_t`.
* Использует три (иногда четыре) гейта, которые управляют чтением/записью в память.
* Благодаря этому может хранить информацию на десятки шагов без сильного затухания градиента.

Простая RNN обновляет состояние по `h_t = tanh(W_x x_t + W_h h_{t-1} + b)`. LSTM делает это сложнее, контролируя, что добавить, что забыть и что вывести.

## Структура LSTM

Основные уравнения:

```
i_t = σ(W_i x_t + U_i h_{t-1} + b_i)        # input gate
f_t = σ(W_f x_t + U_f h_{t-1} + b_f)        # forget gate
g_t = tanh(W_g x_t + U_g h_{t-1} + b_g)     # candidate (иногда называют c̃_t)
o_t = σ(W_o x_t + U_o h_{t-1} + b_o)        # output gate

c_t = f_t ⊙ c_{t-1} + i_t ⊙ g_t             # новое состояние памяти
h_t = o_t ⊙ tanh(c_t)                       # выход
```

`σ` — сигмоида, `⊙` — поэлементное умножение.

## За что отвечают гейты

* **Forget gate (`f_t`)**: решает, какую часть старой памяти `c_{t-1}` оставить. Если `f_t ≈ 1`, память сохраняется; если `≈ 0`, забывается.
* **Input gate (`i_t`)** + **candidate `g_t`**: определяют, какую новую информацию записать в память. `i_t` решает «насколько», `g_t` — что именно.
* **Output gate (`o_t`)**: контролирует, сколько информации из `c_t` попадёт в скрытое состояние/выход `h_t`.

Благодаря этим гейтам ячейка может selectively помнить/забывать, что важно для длинных зависимостей.

## Борьба с затуханием/взрывом градиентов

* **Затухание**: главная фишка LSTM — почти линейное перенесение памяти через `c_t`. Если `f_t ≈ 1`, то `c_t ≈ c_{t-1}`, и градиент проходит через умножение на 1, не уменьшаясь. В простой RNN градиент многократно умножается на `W_h` и tanh′, поэтому быстро стремится к нулю.
* **Взрыв**: LSTM всё равно может страдать, но практики используют gradient clipping. Внутренне гейты ограничивают значения от 0 до 1, что снижает риск неконтролируемого роста. Также наличие отдельного пути памяти помогает избежать накопления больших производных.

В результате LSTM способна запоминать и переносить информацию на сотни шагов, что делает её эффективной для языковых моделей, перевода и других задач с длинными зависимостями.
