# Вопрос №4.03

> Модель bag of words. Чем cbow отличается от skip-gram

## Bag of Words (BoW)

**Определение:** представление текста как мешка слов. Порядок игнорируется, учитываются только частоты или факты появления слов из словаря.

**Как строится:**

1. Составляем словарь `V` из всех уникальных токенов корпуса (иногда после очистки, лемматизации).
2. Для каждого документа формируем вектор длины `|V|`: на позиции `i` стоит либо количество вхождений слова `v_i`, либо признак «слово присутствует».

**Особенности и проблемы:**

* Простота и скорость: никаких обучаемых параметров, достаточно подсчёта.
* Разреженность и большая размерность, особенно при большом словаре или добавлении n-gram.
* Теряется порядок слов и дальние зависимости: фразы «не люблю» и «люблю не» одинаковы.
* Не учитывает сходство слов: разные, но похожие слова получают независимые координаты.
* Хорошо подходит для простых линейных моделей (логистическая регрессия, SVM), особенно когда нужен baseline.

## CBOW vs Skip-gram

Обе модели — варианты word2vec. Цель — обучить плотные эмбеддинги слов, используя локальный контекст.

| Модель | Что предсказывает | Чем отличается |
| --- | --- | --- |
| **CBOW (Continuous Bag of Words)** | По нескольким соседним словам (контекст) предсказывает текущее слово | Быстрее обучается, усредняет эмбеддинги контекста. Хорошо работает на больших корпусах и часто встречающихся словах, но сглаживает редкие. |
| **Skip-gram** | По текущему слову пытается предсказать каждое слово из его окна контекста | Лучше обучает эмбеддинги редких слов, т.к. каждое слово генерирует много пар (word → context). Медленнее (больше тренировочных примеров). |

**Интуиция:** CBOW “усредняет” контекст → восстанавливает центральное слово. Skip-gram “рассылает” информацию от слова → контексту. При обоих подходах веса embedding-слоя становятся искомыми векторными представлениями слов.
