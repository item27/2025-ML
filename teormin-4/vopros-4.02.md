# Вопрос №4.02

> Проблемы кодирования слов: one-hot, stop-слова, формы слов, n-gramms

## One-hot

**Как работает:** каждому слову (токену) из словаря присваивается индекс, и оно кодируется вектором длиной `|V|`, где все элементы равны 0, кроме одного (позиция индекса = 1).

**Проблемы**

* Размерность = размер словаря. Для реальных корпусов это десятки/сотни тысяч измерений, память и вычисления растут линейно.
* Векторы разреженные и не содержат информации о сходстве слов: расстояние между «кот» и «кошка» такое же, как между «кот» и «микроскоп».
* Добавление новых слов требует расширять векторы и переобучать модели.

## Stop-слова

**Что это:** очень частые, но малоинформативные токены (предлоги, союзы). Их часто удаляют перед кодированием.

**Проблемы**

* Если не удалять, они забивают статистику (TF, n-grams) и увеличивают размерность.
* Если удалить, можно потерять важные грамматические маркеры (например, «не» меняет смысл фразы).
* Списки стоп-слов надо адаптировать под домен и язык, иначе будет мусор или потери смысла.

## Формы слов (морфология)

Языки (особенно русский) склоняют и спрягают слова: «кот», «кота», «коту». Если каждую форму считать отдельным токеном, словарь раздувается, а статистика по каждой форме мала.

**Что делают**

* Лемматизация или стемминг — приводят к нормальной форме, уменьшая словарь.
* Но леммы могут терять синтаксическую информацию (падеж, время), что иногда критично.

## n-grams

**Идея:** чтобы учесть локальный порядок слов, к признакам добавляют биграммы, триграммы и т. д.

**Проблемы**

* Размерность растёт комбинаторно: количество уникальных n-gramm огромное, особенно с большими словарями.
* Статистика редких n-gram плохо оценивается → шум.
* Увеличение n слабо помогает моделировать дальние зависимости (n остаётся маленьким).
* При включении n-gram надо аккуратно нормировать веса, иначе частые шаблоны доминируют.

Итог: простые представления легко посчитать, но быстро упираются в разреженность, размер словаря и потерю смысла. Поэтому современные модели переходят к обучаемым эмбеддингам и субсловным токенам.
