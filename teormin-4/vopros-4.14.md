# Вопрос №4.14

> Как устроена модель GPT-3

GPT-3 (Generative Pre-trained Transformer 3) — крупная языковая модель от OpenAI с ~175 млрд параметров. Это авто-регрессионный трансформер (только decoder), обученный предсказывать следующий токен по предыдущим.

## Архитектура

* **Токенизация:** byte pair encoding (50К подслов). Всё приводится к токенам одинакового словаря.
* **Эмбеддинги:** токен-эмбеддинг + позиционный эмбеддинг (обучаемые или sinusoidal — в GPT-3 обучаемые).
* **Только decoder:** множество (96 в базовой версии) одинаковых блоков:
  * LayerNorm → multi-head self-attention (masking запрещает смотреть вперёд).
  * Residual + LayerNorm → позиционно-независимый feed-forward (двухслойный MLP с GELU).
  * Ещё один residual.
* **Output softmax:** линейная проекция в размер словаря и softmax для предсказания следующего токена.

## Обучение

* **Задача:** next-token prediction (language modeling). Функция потерь — кросс-энтропия.
* **Данные:** смесь интернет-текстов (Common Crawl, книги, Википедия) с фильтрацией.
* **Масштаб:** батчи ~3.2М токенов, обучение на нескольких тысячах GPU.
* **Специализация:** модель не fine-tune’ят для отдельных задач; вместо этого используют prompting, few-shot, zero-shot.

## Особенности

* **Масштаб:** 125M → 355M → 1.3B → 6.7B → 13B → 175B параметров (несколько конфигураций). GPT-3 обычно = 175B.
* **Context window:** 2048 токенов.
* **LayerNorm перед attention (pre-LN)** — помогает обучению глубоких слоёв.
* **Sparse attention (в некоторых экспериментах)** — но базовая GPT-3 использует dense.
* **Обучение в стиле autoregressive:** модель получает реальные предыдущие токены (teacher forcing) и учится предсказывать следующий; causal mask запрещает смотреть вперёд.

Итог: GPT-3 — это очень большой, чисто декодерный трансформер, обученный на огромном корпусе для генерации текста. Его сила — в масштабе и широком спектре данных, что позволяет выполнять множество задач через подсказки вместо специализированного обучения.
