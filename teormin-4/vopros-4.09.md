# Вопрос №4.09

> Как с помощью RNN генерировать текст слово за словом

## Общая схема

1. **Обучение:** RNN обучается предсказывать следующий токен по предыдущим. На вход подают последовательность слов (обычно их эмбеддинги), на каждом шаге сеть выдаёт распределение `p(word_t | word_{<t})`. Функция потерь — кросс-энтропия по правильному следующему слову.
2. **Инференс:** после обучения модель может продолжать любой стартовый текст, генерируя слова по одному.

## Процесс генерации

1. **Инициализация:** задаём начальное скрытое состояние (обычно нули) и стартовый токен `<BOS>` или пользовательский prompt.
2. **Шаг вперёд:** подаём текущий токен `x_t` в RNN, обновляем скрытое состояние `h_t`, получаем логиты/вероятности для следующего слова через softmax.
3. **Выбор слова:** из распределения выбираем следующий токен:
   * `argmax` (greedy) — всегда берёт самое вероятное слово.
   * выбор по вероятности (sampling) — для разнообразия.
   * beam search — чтобы искать более глобально вероятные последовательности.
4. **Петля:** добавляем выбранное слово в результат, подаём его обратно как `x_{t+1}`, повторяем шаги 2–3, пока не достигнем `<EOS>` или нужной длины.

## Практические детали

* Работает и для символьного уровня (по буквам) и для слов/подслов.
* Можно управлять «креативностью» через температуру при выборке: делим логиты на `τ`. При `τ > 1` распределение сглаживается → разнообразие; при `τ < 1` — становится острее → более детерминированный результат.
* Для безопасности часто ограничивают максимальную длину генерации или прекращают по специальным токенам.

Таким образом RNN превращает задачу генерации в последовательное применение «предиктора следующего слова», где каждое новое слово зависит от накопленного скрытого состояния, содержащего информацию о предыдущем тексте.
