# Вопрос №4.13

> Attention. Меняется ли размер эмбеддинга слова после применения attention-слоя? Меняется ли сам вектор? Как?

### Размер

В классической реализации (Transformer) размер эмбеддинга после attention **не меняется**. На вход подаётся последовательность векторов размерности `d_model`. Внутри multi-head attention каждый head работает в меньшем пространстве `d_head`, но после конкатенации и линейной проекции (`W^O`) результат снова имеет размер `d_model`. Таким образом, слой внимания совместим по размеру с остаточными связями (residual).

### Сам вектор

Да, **сам вектор меняется**. Attention заменяет исходное представление токена взвешенной суммой value-векторов других токенов (и самого себя). Формально:

```
Z = softmax(QK^T / sqrt(d_k)) V
```

Строка `Z_i` — новый вектор для токена `i`. Это линейная комбинация всех `V_j` с весами `α_{i,j}`. Поэтому

* компоненты вектора теперь содержат информацию из других слов;
* направление/значения координат могут сильно измениться относительно исходного `x_i`, но остаются в том же пространстве.

Дополнительно после attention обычно есть residual connection `x_i + Z_i` и LayerNorm, так что итоговый вектор = исходный плюс attention-поправка, нормализованная. Это сохраняет размерность, но вводит новую информацию, перезаписывая прежнее значение.
