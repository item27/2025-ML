# Вопрос №3.09

> Модель `t-sne`

t-SNE — метод нелинейной визуализации, который уменьшает размерность, сохраняя локальную структуру данных (соседства), но не предназначен для точного сохранения глобальных расстояний.

**Идея в двух словах.** Сопоставить вероятности соседства в высокоразмерном пространстве и в низкоразмерном, затем минимизировать расхождение между ними (KL-дивергенция).

**Высокоразмерные сходства.** Для каждой пары точек определяется условная вероятность

$$
p_{j|i}=\frac{\exp!\big(-|x_i-x_j|^2/(2\sigma_i^2)\big)}{\sum_{k\ne i}\exp!\big(-|x_i-x_k|^2/(2\sigma_i^2)\big)}.
$$

Симметризуют:

$$
p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n}.
$$

Параметр $$\sigma_i$$ подбирают так, чтобы энтропия распределения соответствовала заданной perplexity.

**Низкоразмерные сходства.** В пространстве отображений используется тяжёлый хвост t-распределения (степень свободы = 1):

$$
q_{ij}=\frac{(1+|y_i-y_j|^2)^{-1}}{\sum_{k\ne l}(1+|y_k-y_l|^2)^{-1}}.
$$

**Функция потерь.** Минимизируется KL-дивергенция:

$$
C=\sum_{i\ne j} p_{ij}\log\frac{p_{ij}}{q_{ij}}.
$$

**Градиент (важно для понимания поведения):**

$$
\frac{\partial C}{\partial y_i}=4\sum_j (p_{ij}-q_{ij}),(y_i-y_j),(1+|y_i-y_j|^2)^{-1}.
$$

Отсюда видно, что точки с $$p_{ij}\gg q_{ij}$$ притягиваются сильнее, а с $$p_{ij}\ll q_{ij}$$ отталкиваются.

## **Алгоритм в практике.**

1. Вычислить $$p_{ij}$$ (зависит от perplexity).
2. Инициализировать $$y$$ (случайно или PCA).
3. Минимизировать $$C$$ методом градиентного спуска с моментумом.
4. Применяют «early exaggeration»: временно умножают $$p_{ij}$$ на фактор (обычно ≈12) первые \~250 итераций, чтобы сформировать кластеры.
5. Затем продолжают оптимизацию без exaggeration.

## **Ключевые гиперпараметры и рекомендации.**

* $$\text{perplexity}$$ — отвечает за эффективный размер окрестности. Типичные значения $$5\ldots50$$. Часто выбирают $$\sim30$$. Меньше — мелкие локальные структуры, больше — более глобальные.
* $$\text{learning_rate}$$ — обычно порядка $$10^1\ldots10^3$$ (sklearn по умолчанию 200). Слишком малый тормозит схождение; слишком большой даёт рассыпание точек.
* $$\text{early_exaggeration}$$ — \~12.0 по умолчанию.
* $$\text{n_iter}$$ — минимум 1000; для стабильности 1000–2000 и более.
* Инициализация: PCA часто даёт более стабильный результат, чем случайная инициализация.
* Повторные запуски с разными seed обязательны: t-SNE стохастичен.

## **Сложность и ускорения.**

* Наивная реализация требует всех попарных расстояний → $$O(n^2)$$ по времени и памяти.
* Barnes-Hut t-SNE снижает сложность до примерно $$O(n\log n)$$ для больших $$n$$.
* FIt-SNE и другие реализации дают ещё более быструю аппроксимацию (приближённо линейную по $$n$$).

## **Ограничения и ошибки интерпретации.**

* Сохраняет локальные соседства. Не стоит интерпретировать расстояния между далёкими кластерами как значимые.
* Кластеры на картинке могут зависеть от гиперпараметров и инициализации.
* Нельзя напрямую использовать t-SNE-координаты для большинства downstream-задач без осторожности. Лучше использовать их для визуализации и инспекции.
* Для больших размерностей сначала применять PCA (например, до 30–50 компонент), чтобы убрать шум и ускорить t-SNE.

## **Когда выбирать t-SNE vs alternatives.**

* Для качественной 2D/3D визуализации локальной структуры — t-SNE.
* Для более быстрых, детерминистичных и частично сохраняющих глобальную структуру вложений — UMAP.
* Для линейной проекции и предобработки — PCA.

## **Практический пример (sklearn)**

```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# X подготовлено и масштабировано
X_pca = PCA(n_components=50, random_state=0).fit_transform(X)  # рекомендовано для больших d
tsne = TSNE(n_components=2, perplexity=30, learning_rate=200,
            n_iter=1000, init='pca', random_state=0)
Y = tsne.fit_transform(X_pca)
```

## **Короткая шпаргалка для сдачи.**

* t-SNE моделирует локальные вероятности $$p_{ij}$$ и $$q_{ij}$$ и минимизирует KL.
* Высокоразмерные сходства — гауссовы с адаптивной шириной ($$\sigma_i$$), низкоразмерные — t-распределение (тяжёлый хвост).
* Главное: сохраняется локальная структура, глобальные расстояния ненадёжны.
* Использовать PCA перед t-SNE, пробовать perplexity, фиксировать random\_state и запускать несколько раз.
