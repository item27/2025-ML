# Вопрос №3.06

> Влияние признаков с разным размахом вариации на k-means

Коротко: признаки с большим размахом доминируют в расстоянии Евклида и смещают центры $$k$$-means, поэтому без нормировки кластеризация часто неверна.

## Почему (формула)

$$
|x-\mu|^2=\sum_{j}(x_j-\mu_j)^2.
$$

Если один признак имеет масштаб в 100 раз больше другого, его вклад в сумму квадратов будет порядка $$100^2=10,000$$ раз больше.

Простой численный пример (две точки, два признака). $$x_1=(100,1),; x_2=(0,0).$$ Квадрат расстояния:

$$
|x_1-x_2|^2=(100-0)^2+(1-0)^2=10000+1=10001.
$$

Доля вклада первого признака ≈ $$10000/10001\approx 0.9999$$, второй ≈ $$0.0001$$. То есть второй признак почти не влияет. После стандартизации (центрирование и деление на стандартное отклонение) точки станут

$$
x_1'=(1,1),; x_2'=(-1,-1),
$$

и вклад каждого признака равен (50% / 50%). Это демонстрирует эффект доминирования.

## Что происходит в $$k$$-means

* Центры минимизируют суммарные квадраты отклонений. Большие размахи заставляют алгоритм «подстраиваться» под крупные признаки.
* Малые, но информативные признаки могут оказаться проигнорированы.
* Выбор масштаба влияет на форму кластеров (сферические в масштабе используемой метрики).

## Практические решения (коротко)

1. Проверить размахи и дисперсии признаков.
2. Нормализовать/стандартизовать:
   * StandardScaler (центровать и делить на $$\sigma$$) — когда данные примерно нормальны.
   * MinMaxScaler (в $$[0,1]$$) — когда важны относительные пропорции и нет сильных выбросов.
   * RobustScaler (по медиане и IQR) — при выбросах.
3. Рассмотреть взвешивание признаков если у вас априорная важность.
4. Использовать PCA / sphering (whitening) или Mahalanobis-преобразование для устранения корреляций и выравнивания дисперсий. $$X' = S^{-1/2}(X-\bar X)$$ где $$S$$ — ковариационная матрица.
5. Сравнивать результаты для нескольких вариантов масштабирования по силуету или WSS/BSS.
6. Для категориальных признаков применять специальные кодировки (one-hot) и затем масштабировать / взвешивать, иначе большое количество one-hot признаков тоже доминирует.

## Быстрая проверка в коде (sklearn)

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

scalers = {
  'std': StandardScaler(),
  'minmax': MinMaxScaler(),
  'robust': RobustScaler()
}

for name, scaler in scalers.items():
    Xs = scaler.fit_transform(X)
    km = KMeans(n_clusters=k, n_init=10, random_state=0).fit(Xs)
    print(name, 'silhouette=', silhouette_score(Xs, km.labels_))
```

## Короткая шпаргалка для сдачи

* Причина: евклидова норма суммирует квадраты по признакам; большой масштаб = большой вклад.
* Следствие: без нормировки важные мелкомасштабные признаки игнорируются.
* Решение: масштабирование (Standard/MinMax/Robust), PCA/whitening, проверка по метрикам (силуэт, WSS).
