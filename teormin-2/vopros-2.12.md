# Вопрос №1.12

> Метрики классификации: precision, recall, accuracy, F1, F-beta, roc auc кривая и как она строится, confusion matrix

**Ответ:**

Метрики классификации оценивают качество предсказаний модели.

Особенно важно различать случаи, когда данные **несбалансированы** (например, 95% одного класса и 5% другого).

<figure><img src="../.gitbook/assets/Pasted image 20251013235606.png" alt=""><figcaption></figcaption></figure>

**Accuracy**

Доля правильных предсказаний:

$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

* Простая, понятная метрика.
* Плохо работает при **дисбалансе классов** (можно “наугад” предсказывать самый частый класс и получить высокую точность).

**Precision (точность)**

$$
Precision = \frac{TP}{TP + FP}
$$

Из всех объектов, предсказанных как “положительные”, - **сколько действительно положительные**?

> «Насколько модель уверена в своих положительных предсказаниях».

**Пример:** при фильтрации спама — сколько писем, которые модель пометила как спам, **действительно спам?**

**Recall (полнота)**

$$
Recall = \frac{TP}{TP + FN}
$$

Из всех реальных положительных объектов — **сколько модель нашла?**

> «Насколько хорошо модель находит все случаи положительного класса».

**Пример:** при диагностике болезней — сколько больных людей модель действительно определила как больных?

**F1-score**

$$
F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
$$

Это **гармоническое среднее** между Precision и Recall.

Используется, когда важно найти **баланс** между пропусками и ложными срабатываниями.

> F1 высокий только тогда, когда и Precision, и Recall высоки.

**Fβ-score**

$$
F_\beta = (1 + \beta^2) \cdot \frac{Precision \cdot Recall}{(\beta^2 \cdot Precision) + Recall}
$$

Позволяет **задать приоритет**:

* $$\beta > 1$$ — важнее **Recall** (лучше поймать всех, даже ценой FP);
* $$\beta < 1$$ — важнее **Precision** (лучше меньше FP, но увереннее).

**ROC-кривая (Receiver Operating Characteristic)**

<figure><img src="../.gitbook/assets/Pasted image 20251014000055.png" alt=""><figcaption></figcaption></figure>

ROC-кривая показывает, **как меняются качество классификации при разных порогах** вероятности.

**По осям:**

* X: **FPR** = FP / (FP + TN) — доля ложных срабатываний;
* Y: **TPR (Recall)** = TP / (TP + FN) — доля правильно найденных положительных.

**Процесс:**

1. Модель выдаёт вероятности (например, 0.1, 0.7, 0.9…);
2. Меняем порог классификации от 0 до 1;
3. Для каждого порога считаем (FPR, TPR);
4. Строим кривую ROC.

**AUC (Area Under Curve)**

<figure><img src="../.gitbook/assets/Pasted image 20251014000201.png" alt=""><figcaption></figcaption></figure>

**AUC ROC** = площадь под ROC-кривой (от 0 до 1).

| AUC значение | Интерпретация                  |
| ------------ | ------------------------------ |
| 0.5          | модель как случайный выбор     |
| > 0.7        | неплохая модель                |
| > 0.9        | отличная модель                |
| 1.0          | идеальная (скорее переобучена) |

**Интуитивно:**

AUC = вероятность, что модель поставит более высокую вероятность положительному объекту, чем отрицательному.

**Многоклассовая классификация**

Если классов больше двух:

* **macro average** — считаем метрику для каждого класса, затем усредняем (все классы равны);
* **micro average** — считаем общие TP, FP, FN по всем классам (учитывает дисбаланс).

**Краткое сравнение:**

| Метрика              | Показывает              | Учитывает дисбаланс | Когда полезна                        |
| -------------------- | ----------------------- | ------------------- | ------------------------------------ |
| **Accuracy**         | Общая доля верных       | -                   | Когда классы сбалансированы          |
| **Precision**        | Чистоту предсказаний    | +                   | Когда важна уверенность (спам, фрод) |
| **Recall**           | Полноту обнаружений     | +                   | Когда важно не пропустить (медицина) |
| **F1**               | Баланс precision/recall | +                   | При дисбалансе классов               |
| **ROC-AUC**          | Качество ранжирования   | +                   | При любых дисбалансах                |
| **Confusion matrix** | Подробный разбор ошибок | +                   | Для анализа поведения модели         |
