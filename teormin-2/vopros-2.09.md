# Вопрос №1.09

> Как вычисляется cross-entropy и почему она cross?

**Ответ:**

**Cross-Entropy Loss** - это функция потерь, которая измеряет, насколько сильно распределение предсказанных вероятностей модели отличается от истинного распределения меток.

**Формула (бинарная классификация)**

$$
L = - \frac{1}{n} \sum_{i=1}^{n} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]
$$

где:

* $$y_i \in \{0, 1\}$$ - истинная метка,
* $$p_i = \sigma(x_i)$$ - предсказанная вероятность класса 1,
* $$\sigma(x)$$ - сигмоида, преобразующая логиты в вероятности.

Если модель уверена и права — потеря близка к 0.

Если модель уверена, но ошибается — потеря сильно растёт.

**Для многоклассовой классификации**

$$
L = - \sum_{c=1}^{M} y_{o,c} \log(p_{o,c})
$$

где:

* $$y_{o,c}$$ - индикатор (1, если объект $$o$$ принадлежит классу $$c$$);
* $$p_{o,c}$$ - вероятность, что модель отнесёт объект $$o$$ к классу $$c$$;
* $$M$$ - количество классов.

**Почему она называется&#x20;**_**cross**_**-entropy?**

“Cross” - потому что это **перекрёстная энтропия между двумя распределениями**:

$$
H(p, q) = -\sum p(x) \log q(x)
$$

* $$p(x)$$ - _истинное распределение_ (ground truth, “one-hot” метки);
* $$q(x)$$ - _предсказанное распределение_ модели.

То есть мы **измеряем энтропию истинного распределения, используя вероятности модели** - “перекрёстно”.

Если бы $$p=q$$, то это была бы просто **энтропия**:

$$
H(p) = -\sum p(x) \log p(x)
$$

**Интуитивно:**

* Если истинная метка **y=1**, то потеря $$-\log(p)$$ быстро растёт, когда $$p$$ близко к 0
* Если **y=0**, то берём вторую часть формулы $$-\log(1 - p)$$

<figure><img src="../.gitbook/assets/Pasted image 20251013230106.png" alt=""><figcaption></figcaption></figure>

Когда совместить две кривые (для y=0 и y=1), они образуют **“крест” (cross)** - отсюда и визуальное объяснение термина:

модель “перекрещивает” два сценария: ошибку для класса 0 и для класса 1.

**Почему это хорошая функция потерь:**

* Наказывает уверенные, но неверные предсказания сильнее, чем неуверенные;
* Стимулирует модель выдавать **реалистичные вероятности**;
* Является гладкой и дифференцируемой - подходит для градиентного спуска;
* Прямо связана с максимизацией правдоподобия (Maximum Likelihood Estimation).
