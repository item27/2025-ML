# Вопрос №1.18

> Модель логистическая регрессия

### Логистическая регрессия (Logistic Regression)

**Идея:**\
Логистическая регрессия — это **линейная модель для классификации**,\
которая предсказывает **вероятность**, что объект относится к какому-то классу (например, “1” или “Да”).

***

### Как работает

1.  Сначала, как в линейной регрессии, считаем **линейную комбинацию признаков**:

    $$
    z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
    $$
2.  Потом прогоняем это через **сигмоиду** (функцию, которая “сжимает” любое число в диапазон от 0 до 1):

    $$
    \hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
    $$
3. Получаем **вероятность**:
   * ближе к 1 → объект скорее относится к классу “1” (например, “положительный”);
   * ближе к 0 → к классу “0” (“отрицательный”).
4. Если вероятность > 0.5 → предсказываем “1”, иначе — “0”.

Пример:\
Модель предсказывает, купит ли клиент товар.\
Если вероятность 0.8 — “да, купит”, если 0.2 — “нет”.

***

#### Как подбираются веса (обучение)

* Модель ищет такие веса $$w$$, чтобы предсказанные вероятности **лучше совпадали с реальными метками** (0 и 1).
* Для этого минимизируется **функция потерь — кросс-энтропия (log-loss)** с помощью **градиентного спуска**.

Проще говоря:

> Модель делает прогноз, видит ошибку, подправляет веса,\
> и так много раз — пока не научится хорошо разделять классы.

***

### Почему “логистическая”

Потому что используется **логистическая функция (сигмоида)** —\
её график выглядит как плавная “S”-образная кривая,\
которая превращает любое число в вероятность от 0 до 1.

***

### Нелинейности и регуляризация

* По сути, модель **линейная**, но на выходе **нелинейно сжимает** результат в \[0;1].
* Чтобы добавить настоящие нелинейные зависимости,\
  можно добавить в признаки квадраты, логарифмы и т. п.
* Регуляризация (L1, L2) — работает точно так же, как в линейной регрессии,\
  чтобы избежать переобучения и убрать лишние признаки.

***

### Плюсы и минусы

**Плюсы:**

* Простая, быстрая, хорошо интерпретируется;
* Даёт вероятности (а не просто “да/нет”);
* Работает даже при малом количестве данных.

**Минусы:**

* Плохо ловит сложные (нелинейные) зависимости;
* Плохо работает, если классы сильно перекрываются;
* Может требовать нормализации признаков.

***

**Итог:**

> Логистическая регрессия — это линейная модель,\
> которая “через сигмоиду” превращает предсказания в вероятность класса.\
> Простая, понятная и одна из базовых моделей классификации.
