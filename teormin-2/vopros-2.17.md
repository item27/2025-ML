# Вопрос №1.17

> Модель линейная регрессия

**Идея:**\
Найти **линейную зависимость** между признаками (X) и целевой переменной (Y).\
Модель пытается “провести прямую” (или гиперплоскость), которая **лучше всего приближает** реальные данные.

$$
\hat{y} = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n
$$

где

* $$\hat{y}$$ — предсказание модели,
* $$x_i$$ — признаки,
* $$w_i$$ — веса (коэффициенты),
* $$w_0$$ — свободный член (сдвиг, bias).

***

### Как обучается модель

Модель подбирает такие веса $$w$$, чтобы **ошибка между предсказанием и реальными данными была минимальна**.

Обычно используется **MSE (Mean Squared Error)**:

$$
MSE = \frac{1}{n} \sum (y_i - \hat{y_i})^2
$$

То есть мы ищем:

$$
\min_w \; MSE(y, Xw)
$$

***

### Как найти веса

1.  **Аналитически (через формулу):**

    $$
    w = (X^T X)^{-1} X^T y
    $$

    даёт точное решение,\
    но требует обращения матрицы → дорого и нестабильно на больших данных.
2. **Чаще — градиентным спуском:**
   * Вычисляем градиент ошибки по w;
   *   Обновляем веса:

       $$
       w = w - \eta \cdot \nabla_w MSE
       $$
   * Повторяем, пока ошибка не перестанет уменьшаться.

***

### Регуляризация

Чтобы модель **не переобучалась**, добавляют штраф за большие веса:

* **L1 (Lasso)** — зануляет неважные признаки;
* **L2 (Ridge)** — уменьшает влияние всех признаков.

$$
\text{Loss} = MSE + \lambda \|w\|_1 \text{ или } MSE + \lambda \|w\|_2^2
$$

***

### Учёт смещения (bias)

Чтобы модель могла “сдвигать” прямую вверх/вниз,\
в матрицу X добавляют **столбец единиц**.\
Это позволяет обучить **свободный член** $$w_0$$.

***

### Может ли линейная регрессия находить нелинейные зависимости?

Формально — **нет**, она ищет **только линейную зависимость**.\
НО можно добавить в признаки **нелинейные преобразования**:

* квадраты: (x\_1^2, x\_2^2)
* произведения: (x\_1 \times x\_2)
* синусы, логарифмы и т.д.

→ Так получится **полиномиальная регрессия** — по сути, линейная модель, но в “расширенном” пространстве признаков.\
Однако при этом модель часто **переобучается**, если добавить слишком много таких фич.

***

### Плюсы и минусы

**Плюсы:**

* Простая и быстрая;
* Хорошо интерпретируется (можно объяснить вклад каждого признака);
* Работает даже на малых данных;
* Устойчива к переобучению (при умеренном числе признаков).

**Минусы:**

* Плохо моделирует нелинейные зависимости;
* Чувствительна к выбросам и мультиколлинеарности (сильно коррелирующим признакам);
* Требует нормализации данных для корректных весов.

***

**Итог:**

> Линейная регрессия ищет “лучшую прямую”, которая минимизирует ошибку между предсказанием и реальностью.\
> Можно решать аналитически или через градиентный спуск,\
> добавлять регуляризацию для борьбы с переобучением\
> и нелинейные признаки для большей гибкости.
