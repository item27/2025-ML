# Вопрос №1.19

### Модель "Дерево решений" (Decision Tree)

Идея:\
Модель последовательно делит данные по признакам так, чтобы увеличить “чистоту” в каждом подмножестве.\
Каждый внутренний узел — это вопрос (условие),\
каждая ветка — возможный ответ,\
а листья — конечные прогнозы (класс или число).

***

### Как работает дерево решений

1. В корне выбирается признак, который лучше всего разделяет выборку.
2. Данные делятся на подмножества по этому признаку (ветвление).
3. Процесс повторяется рекурсивно, пока не достигнуто:
   * максимальная глубина,
   * минимальное количество объектов в листе,
   * или чистота подмножества (все объекты одного класса).

***

### Критерии разбиения

Цель — максимизировать "информационный выигрыш" (Information Gain).\
Это измеряет, насколько хорошо признак разделяет данные.

| Тип задачи    | Критерий            | Формула / интуиция                                     |
| ------------- | ------------------- | ------------------------------------------------------ |
| Классификация | Gini impurity       | $$G = 1 - \sum p_i^2$$ (чем меньше, тем “чище”)        |
| Классификация | Entropy             | $$H = - \sum p_i \log_2(p_i)$$                         |
| Регрессия     | MSE / MAE reduction | деление выбирается по минимальной ошибке в поддеревьях |

***

### Преимущества дерева

Интерпретируемая модель ("если ... то ...")\
Работает без нормализации признаков\
Может работать с категориальными фичами и нелинейностями

Склонна к переобучению, особенно при большой глубине\
Нестабильна — небольшое изменение данных может сильно изменить дерево

***

### Модель "Случайный лес" (Random Forest)

Random Forest = ансамбль из многих деревьев решений, обученных на разных подвыборках данных и признаков.

Идея:

> “Пусть каждое дерево ошибается по-своему, а при голосовании ошибки взаимно компенсируются.”

***

### Как работает случайный лес

1. Из исходных данных создаются случайные подвыборки (с возвращением) — bagging.
2. Для каждого дерева случайно выбирается подмножество признаков.
3. Каждое дерево строится независимо.
4. Итоговое предсказание:
   * для классификации — голосование большинства;
   * для регрессии — усреднение предсказаний.

* Классификация: прирост информации (энтропия) или снижение индекса Джини.
* Регрессия: снижение MSE/MAE.

Устойчив к переобучению\
Хорошо работает “из коробки”\
Даёт оценку важности признаков\
Труднее интерпретировать, чем одно дерево\
Медленнее при большом количестве деревьев

***

* **MDI (impurity-based):** суммарное снижение нечистоты, внесённое признаком, усреднённое по деревьям.
* **Permutation importance:** рост ошибки после случайной перестановки значений признака. Менее смещена, дороже по вычислениям.

#### 1. По уменьшению критерия (Gini / Entropy)

Наиболее распространённый способ (в sklearn по умолчанию):

1. Для каждого узла дерева считается, насколько улучшилось качество после разбиения (например, уменьшилась Gini impurity).
2. Это улучшение (gain) приписывается признаку, по которому было разбиение.
3. Вклад всех узлов по признаку суммируется и нормируется.

$$
Importance(feature_j) = \frac{\sum_{nodes \ where \ split=j} (impurity_{parent} - impurity_{left} - impurity_{right}) \times samples_{node}}{\text{total gain}}
$$

Быстро и наглядно\
Может переоценивать признаки с большим количеством уникальных значений (например, числовые фичи)

***

#### 2. По уменьшению точности при перестановке (Permutation Importance)

* После обучения перемешивают значения признака j (разрушают связь с таргетом);
* Считают, насколько ухудшилось качество модели;
* Чем сильнее ухудшение — тем важнее признак.

Более честная оценка\
Дольше вычисляется

***

### Интуитивно

Если удалить или “испортить” важный признак — дерево начнёт ошибаться гораздо чаще.\
Если ошибка почти не изменилась — признак незначим.

***

Итог:

> Дерево решений — интерпретируемая, но нестабильная модель.\
> Случайный лес — ансамбль деревьев, который снижает переобучение.\
> Feature importance показывает, какие признаки чаще и сильнее влияют на разбиения.
