# Вопрос №6.04

> Как получить мел-кепстральные коэффициенты (mfcc) для аудиозаписи? Можно ли их использовать в качестве эмбеддингов?

### Как демонстративно получить MFCC для аудиозаписи

MFCC (Mel-Frequency Cepstral Coefficients) — компактные признаки, которые описывают **спектральную форму** (в духе “тембра/формант”) в каждом коротком окне.

Типичный конвейер:

1. **Предобработка**

* при необходимости приводим сигнал к одной $$f_s$$ (часто $$16000$$ Гц),
* делаем нормализацию амплитуды (не обязательно, но полезно).

2. **Разбиение на окна**

* длина окна $$\approx 25$$ мс, шаг $$\approx 10$$ мс.
* то есть $$win_length \approx 0.025\cdot f_s$$, $$hop \approx 0.010\cdot f_s$$.

3. **Окно + FFT**

* умножаем каждый фрейм на окно $$w[n]$$ (часто Hann),
* считаем спектр мощности: $$P[k]=|X[k]|^2$$

4. **Mel filter bank**

* применяем набор мел-фильтров (например, $$n_{mels}=40$$): получаем энергии по мел-каналам $$E[i]$$.

5. **Логарифм**

* берём лог-энергии (часто в натуральном логе или $$\log_{10}$$): $$L[i]=\log(E[i]+\epsilon)$$

6. **DCT (дискретное косинусное преобразование)**

* применяем DCT к $$L[i]$$ и берём первые $$n_{mfcc}$$ коэффициентов (обычно $$13$$): $$c[p]=\mathrm{DCT}(L)[p],\quad p=0,\dots,n_{mfcc}-1$$

7. **Опционально: дельты**

* скорость изменения признаков: $$\Delta c,\ \Delta^2 c$$ Часто итоговый вектор на фрейм: $$13 + 13 + 13 = 39$$.

**Результат:** матрица $$T \times n_{mfcc}$$ (или $$T \times 39$$), где $$T$$ — число временных окон.

***

### Пример получения (Python)

#### librosa

```python
import librosa
import numpy as np

y, sr = librosa.load("audio.wav", sr=16000)

mfcc = librosa.feature.mfcc(
    y=y,
    sr=sr,
    n_mfcc=13,
    n_fft=512,
    hop_length=int(0.010 * sr),
    win_length=int(0.025 * sr),
    n_mels=40
)  # shape: (13, T)

mfcc = mfcc.T  # (T, 13)
```

#### torchaudio

```python
import torch
import torchaudio

waveform, sr = torchaudio.load("audio.wav")  # (ch, n)
if sr != 16000:
    waveform = torchaudio.functional.resample(waveform, sr, 16000)
    sr = 16000

mfcc_tf = torchaudio.transforms.MFCC(
    sample_rate=sr,
    n_mfcc=13,
    melkwargs={
        "n_fft": 512,
        "win_length": int(0.025 * sr),
        "hop_length": int(0.010 * sr),
        "n_mels": 40,
        "power": 2.0,
    },
)

mfcc = mfcc_tf(waveform)   # (ch, 13, T)
mfcc = mfcc[0].transpose(0, 1)  # (T, 13)
```

***

### Можно ли использовать MFCC как эмбеддинги?

Да, но важно различать “признаки” и “эмбеддинги”.

#### 1) MFCC как эмбеддинги в простом смысле

MFCC — это уже **векторное представление** аудио по времени. Их можно:

* усреднить по времени: $$e=\mathrm{mean}_t;c[t]$$
* или собрать статистики: $$e=[\mathrm{mean}_t,\ \mathrm{std}_t]$$
* или прогнать через простую модель (SVM / логрег / MLP / LSTM) и уже её скрытый слой считать эмбеддингом.

Это работает для:

* простых классификаций (есть/нет звук),
* грубой кластеризации,
* задач, где не нужен “семантический” уровень.

#### 2) Ограничения MFCC как “универсальных эмбеддингов”

MFCC хорошо кодируют **спектральную форму**, но:

* они не являются “обученными” под смысл/инвариантность,
* хуже переносятся между доменами (разные микрофоны/шумы),
* сильно зависят от параметров (окна, $$n_{mels}$$, нормализации).

Для задач уровня “похожие звуки”, “кто говорит”, “что за событие” обычно лучше:

* log-mel спектрограммы + CNN,
* или готовые self-supervised эмбеддинги (wav2vec2, HuBERT и т.п.).

***

### Практический вывод

* **Да**, MFCC можно использовать как эмбеддинги, особенно если сделать агрегацию по времени (mean/std) и подавать в классический ML.
* Если нужна **устойчивость и переносимость**, лучше использовать **обученные эмбеддинги**, а MFCC оставить как baseline/быстрые признаки.
