# Вопрос №8.01

> Что такое латентное пространство и latent space interpolation

### Латентное пространство (latent space)

Латентное пространство — это “внутренний” набор чисел, в котором модель хранит сжатое описание объекта.

* В обычном мире у нас есть **данные**: картинка (миллионы пикселей), звук (длинный сигнал), текст (цепочка токенов).
* Модели часто переводят данные в **вектор признаков** — например, 128 или 512 чисел. Этот вектор старается содержать **самую важную информацию** про объект, но в более компактном виде.
* Вот пространство всех таких векторов и называется **латентным пространством**.

#### Почему “латентное”

“Latent” = скрытый. Эти признаки не задаются человеком напрямую (“это кот”, “это собака”), а **выучиваются моделью** так, чтобы решать задачу: генерировать, классифицировать, искать похожее и т.д.

#### Интуиция

Представь, что картинка лица описывается не пикселями, а ручками:

* “улыбка”
* “поворот головы”
* “освещение”
* “цвет волос”
* “возраст”

В реальности модель не хранит эти параметры такими словами, но **пытается устроить латентное пространство так, чтобы похожие объекты были рядом**.

#### Где это встречается

* **Автоэнкодер (AE)**: encoder переводит $$x \rightarrow z$$, decoder переводит $$z \rightarrow \hat{x}$$. Здесь $$z$$ — латентный вектор.
* **VAE**: вместо одного $$z$$ модель учит распределение $$q(z|x)$$ и обычно предполагает prior $$p(z)=\mathcal{N}(0, I)$$.
* **GAN / Diffusion**: часто тоже есть “скрытый код” $$z$$ (в GAN напрямую; в diffusion латентным может быть пространство латентного диффузионного автоэнкодера).

***

### Latent space interpolation (интерполяция в латентном пространстве)

Интерполяция — это “плавный переход” между двумя объектами через их латентные коды.

#### Идея

Берём два объекта:

* объект A с кодом $$z_A$$
* объект B с кодом $$z_B$$

Дальше строим промежуточные коды $$z(t)$$, где $$t \in [0,1]$$:

* при $$t=0$$ получаем $$z_A$$
* при $$t=1$$ получаем $$z_B$$
* между ними — “смешанные” варианты

Самый простой вариант — линейная интерполяция (LERP): $$z(t) = (1 - t), z_A + t, z_B$$

Потом подаём $$z(t)$$ в декодер/генератор и получаем промежуточные объекты: $$\hat{x}(t) = decoder(z(t))$$

#### Зачем это делают

1. **Показать, что пространство “осмысленное”**: если при переходе картинка меняется плавно, значит латентные признаки устроены разумно.
2. **Генерация новых вариантов**: можно получать “смешанные” примеры между двумя точками.
3. **Понимание факторов**: если движение в одном направлении меняет, например, “улыбку”, это признак, что модель выделила этот фактор.

#### Почему лучше интерполировать в латентном, а не в данных

Если смешивать пиксели двух лиц напрямую, получится “двойная экспозиция”. А в хорошем латентном пространстве переход часто выглядит как **плавное изменение смысла**: поворот головы, изменение выражения лица и т.п.

***

### Важный нюанс: LERP vs SLERP

В некоторых моделях (особенно где prior — нормальное распределение) латентные векторы обычно лежат “на сфере” по длине примерно одинаково. Тогда линейная интерполяция может “проваливаться” в область, где модель редко бывала.

Для этого используют сферическую интерполяцию (SLERP) — движение по дуге сферы между $$z_A$$ и $$z_B$$. Смысл простой: держим длину вектора более стабильной, чтобы оставаться в “типичных” областях латентного пространства.

***

### Мини-пример (простая аналогия)

Пусть модель кодирует звук:

* $$z_A$$ — “голос ребёнка, плач”
* $$z_B$$ — “тишина / шум комнаты”

Интерполяция даст промежуточные состояния: плач становится тише, добавляется шум, тембр меняется — то есть **плавный переход**.

***

### Критерий “хорошей” интерполяции

Хорошее латентное пространство даёт:

* плавные изменения результата при плавном изменении $$z$$
* логичные промежуточные варианты (без резких скачков и артефактов)

Плохое — когда в середине получается мусор или резкие “переключения”.

