# Вопрос №8.03

> Модель AutoEncoder. Что такое bottleneck, encoder, decoder, reconstruction error. Как обучить генерации mnist картинок

### AutoEncoder (AE)

AutoEncoder — это нейросеть, которая учится **сжимать данные и восстанавливать обратно**.

Состоит из двух частей:

* **Encoder** $$E$$: переводит вход $$x$$ в компактный код $$z$$ $$z = E(x)$$
* **Decoder** $$D$$: восстанавливает вход из кода $$z$$ $$\hat{x} = D(z)$$

Здесь:

* $$x$$ — реальная картинка (MNIST)
* $$\hat{x}$$ — восстановленная картинка
* $$z$$ — латентный вектор (сжатое представление)

***

### Bottleneck (узкое горлышко)

**Bottleneck** — это место, где размерность представления резко меньше, чем у входа.

Например:

* вход MNIST: $$28\cdot 28 = 784$$ чисел
* bottleneck: $$z$$ размером 16–64 чисел

Зачем это нужно:

* если не сделать “узко”, сеть может просто выучить копирование (почти тождественную функцию)
* bottleneck заставляет сеть **выделять главное**: форму цифры, толщину, наклон и т.д.

***

### Reconstruction error (ошибка реконструкции)

Это функция потерь, которая измеряет, насколько $$\hat{x}$$ похож на $$x$$.

Частые варианты:

#### 1) MSE (среднеквадратичная)

$$L = |x - \hat{x}|^2$$ Хорошо работает, если пиксели нормализованы как числа.

#### 2) BCE (binary cross-entropy)

Используют, если пиксели в $$[0,1]$$ и выход декодера — $$sigmoid$$: $$L = -\sum_i \big(x_i \log \hat{x}_i + (1-x_i)\log(1-\hat{x}_i)\big)$$

Смысл простой: **минимизируем разницу между исходной и восстановленной картинкой**.

***

### Как обучить AE на MNIST (восстановление)

#### 1) Данные

* берём MNIST
* нормализуем в $$[0,1]$$ (если BCE + sigmoid) или в $$[-1,1]$$ (если tanh)

#### 2) Архитектура (простая MLP)

**Encoder**:

* $$784 \rightarrow 256 \rightarrow 64 \rightarrow z$$ (например, $$z=32$$)

**Decoder**:

* $$z \rightarrow 64 \rightarrow 256 \rightarrow 784$$
* на выходе: $$sigmoid$$ (если пиксели $$[0,1]$$)

#### 3) Обучение

Для каждого batch:

1. $$z = E(x)$$
2. $$\hat{x} = D(z)$$
3. $$L = reconstruction_loss(x, \hat{x})$$
4. backprop, обновляем параметры $$E$$ и $$D$$

***

### Важно: AE “генерирует” не так, как GAN

Обычный AE отлично умеет:

* восстанавливать $$x \rightarrow \hat{x}$$
* делать интерполяции в $$z$$

Но “генерировать новые цифры из шума” **сложно**, потому что:

* он не заставляет $$z$$ иметь красивое и плотное распределение
* если взять случайный $$z$$, декодер может выдать мусор, потому что такой $$z$$ модель никогда не видела

#### Что можно сказать на защите

* **AE**: хорошо для сжатия/денойзинга/поиска похожих
* **генерация** “из воздуха” лучше получается у **VAE** или **GAN**

***

### Как всё-таки генерировать MNIST с AutoEncoder

Есть 3 рабочих подхода (от простого к правильному).

#### Подход A — “генерация” через интерполяцию

1. Берём две реальные картинки $$x_1, x_2$$
2. Кодируем: $$z_1=E(x_1),; z_2=E(x_2)$$
3. Интерполируем: $$z(t) = (1-t)z_1 + tz_2$$
4. Декодируем $$D(z(t))$$ — получаем плавный переход между цифрами

Это считается генерацией “новых” примеров (они не из датасета), но стартуем от реальных.

#### Подход B — подобрать распределение в $$z$$ и сэмплировать

1. Прогоняем много MNIST через encoder и собираем коды $$z$$
2. Приближаем их распределение простым способом:

* самый простой вариант: считаем среднее $$\mu$$ и дисперсию $$\sigma$$ по каждому измерению и сэмплируем $$z \sim \mathcal{N}(\mu, \sigma^2)$$ или обучаем GMM/Kernel Density (это уже сложнее)

3. Декодируем $$D(z)$$

Минус: качество часто хуже, чем у VAE/GAN.

#### Подход C — Denoising AutoEncoder (лучше по смыслу)

Обучаем AE восстанавливать исходную картинку из зашумлённой:

* подаём $$\tilde{x} = x + \epsilon$$
* восстанавливаем $$\hat{x} = D(E(\tilde{x}))$$
* loss: между $$x$$ и $$\hat{x}$$

Это делает пространство “более гладким”, и при небольших случайных сдвигах в $$z$$ или входе получается более стабильная генерация.

***

### Если нужен “правильный генератор” — VAE (одной фразой)

В **VAE** мы явно заставляем латентное распределение быть близким к $$\mathcal{N}(0,I)$$, добавляя KL-штраф: $$L = L_{recon} + \beta \cdot KL(q(z|x)|p(z))$$ Тогда можно честно генерировать: $$z \sim \mathcal{N}(0,I)$$ и $$x=D(z)$$.

***

### Короткий рецепт “AE для MNIST” (что показать в ноутбуке)

1. Обучить AE на reconstruction loss (BCE или MSE)
2. Показать:

* несколько $$x$$ и $$\hat{x}$$ рядом
* 2D/TSNE визуализацию $$z$$ (если $$z$$ маленький)
* интерполяцию между двумя цифрами через $$z(t)$$

3. Если требуют именно “генерацию”:

* показать сэмплинг $$z \sim \mathcal{N}(\mu,\sigma^2)$$ по статистике кодов и результаты декодера
* и честно написать, что VAE делает это корректнее
