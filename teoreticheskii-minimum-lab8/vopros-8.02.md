# Вопрос №8.02

> Модель GAN. Что такое adversarial loss, discriminator, generator. Как обучить GAN генерации mnist картинок

### GAN (Generative Adversarial Network)

GAN — это генеративная модель, которая учится **делать новые данные**, похожие на реальные (например, новые цифры как в MNIST), не копируя их напрямую.

Состоит из двух нейросетей:

* **Generator** (генератор) $$G$$: делает “фейковые” примеры Вход: случайный шум $$z$$ (например, 100 чисел) Выход: сгенерированная картинка $$x_{fake}=G(z)$$
* **Discriminator** (дискриминатор) $$D$$: отличает реальное от фейка Вход: картинка $$x$$ Выход: число $$D(x)\in[0,1]$$ — “вероятность, что это реальное”

Интуиция: генератор — “фальшивомонетчик”, дискриминатор — “эксперт”.

***

### Adversarial loss (соревновательная функция потерь)

Обучение построено как игра:

* $$D$$ хочет правильно отличать real от fake
* $$G$$ хочет обмануть $$D$$, чтобы $$D(G(z))$$ было близко к 1

Классическая цель (из оригинальной статьи GAN) — minimax:

$$\min_G \max_D ; \mathbb{E}*{x\sim p*{data}}[\log D(x)] + \mathbb{E}_{z\sim p(z)}[\log (1-D(G(z)))]$$

Разложим на две части (как это делают на практике):

#### Loss для дискриминатора

Он хочет:

* на реальных: $$D(x_{real}) \rightarrow 1$$
* на фейковых: $$D(x_{fake}) \rightarrow 0$$

Обычно берут:

$$L_D = -\mathbb{E}*{x\sim p*{data}}[\log D(x)] - \mathbb{E}_{z\sim p(z)}[\log(1 - D(G(z)))]$$

(минус потому что оптимизируем градиентным спуском)

#### Loss для генератора

Есть два популярных варианта.

**1) “Сатурирующий” (из minimax):** $$L_G = \mathbb{E}_{z\sim p(z)}[\log(1 - D(G(z)))]$$ Проблема: если $$D$$ слишком хорош, градиенты могут быть маленькие.

**2) “Нон-сатурирующий” (чаще используют):** $$L_G = -\mathbb{E}_{z\sim p(z)}[\log D(G(z))]$$ Смысл: напрямую толкаем $$D(G(z))$$ к 1. Обучение обычно стабильнее.

***

### Что делают Generator и Discriminator

#### Discriminator $$D$$

* обычная бинарная классификация: “реальное / фейк”
* архитектура: CNN (для картинок) или MLP (на MNIST тоже можно, но CNN лучше)

#### Generator $$G$$

* “декодер”: из шума строит картинку
* архитектура: MLP или “deconvolution” (ConvTranspose) как в DCGAN

***

### Как обучить GAN на MNIST (пошагово)

#### 1) Подготовить данные

* MNIST: картинки $$28\times 28$$, значения пикселей лучше привести к диапазону $$[-1,1]$$ (если в конце генератора $$tanh$$)

#### 2) Определить шум

* шум $$z \sim \mathcal{N}(0, I)$$ или равномерный $$U(-1,1)$$
* размерность, например, 64 или 100

#### 3) Архитектуры (минимально понятный вариант на MLP)

**Generator**:

* вход: $$z$$ размером 100
* несколько слоёв
* выход: 784 (это $$28\cdot 28$$), потом reshape в $$1\times 28\times 28$$
* активация на выходе: $$tanh$$

**Discriminator**:

* вход: картинка 784
* несколько слоёв
* выход: 1
* активация на выходе: $$sigmoid$$ (если используем BCE loss)

#### 4) Цикл обучения (главная идея)

На каждой итерации делаем **два шага**.

**Шаг A — обучаем** $$D$$

1. берём batch реальных картинок $$x_{real}$$
2. генерируем batch фейковых: $$x_{fake}=G(z)$$ (обычно **detach**, чтобы не обновлять $$G$$ на этом шаге)
3. считаем $$L_D$$ и обновляем только параметры $$D$$

**Шаг B — обучаем** $$G$$

1. снова берём новый $$z$$
2. генерируем $$x_{fake}=G(z)$$
3. считаем $$L_G$$ (нон-сатурирующий) и обновляем только параметры $$G$$

#### 5) Важные практические настройки (чтобы заработало “с первого раза”)

* оптимизатор Adam: learning rate примерно $$2\cdot 10^{-4}$$
* $$\beta_1=0.5$$ (часто помогает GAN)
* batch size 128 или 64
* 1 шаг $$D$$ на 1 шаг $$G$$ (для простого GAN на MNIST обычно достаточно)
* периодически сохранять сетку сэмплов из фиксированного $$z_{fixed}$$, чтобы видеть прогресс

***

### Мини-псевдокод обучения

1. Инициализируем $$G, D$$
2. Для каждой эпохи:

* Для каждого batch $$x_{real}$$:
  * **update** $$D$$ по $$L_D$$ на real и fake
  * **update** $$G$$ по $$L_G$$, чтобы fake выглядел “real” для $$D$$

***

### Типичные проблемы (и как их назвать на защите)

* **Mode collapse**: генератор начинает выдавать почти одинаковые цифры (например, только “3”). Причина: генератор нашёл “дыру” в дискриминаторе.
* **Дискриминатор слишком сильный**: $$D$$ быстро становится идеальным, и $$G$$ не получает полезных градиентов. Лечат балансом lr, архитектурой, label smoothing, шумом, WGAN и т.д.
* **Нестабильность**: loss может прыгать — это нормально для GAN, важнее качество сэмплов.

***

### Если нужно “как в учебниках” (DCGAN для MNIST)

* $$D$$: несколько Conv + LeakyReLU
* $$G$$: ConvTranspose + BatchNorm + ReLU, на выходе $$tanh$$ DCGAN обычно даёт более красивые цифры и обучается стабильнее, чем чистый MLP.
