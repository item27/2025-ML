# Вопрос №8.04

> Модель Variational AutoEncoder. Чем отличается от AutoEncoder, как генерирует латентный вектор

### Variational AutoEncoder (VAE)

VAE — это автоэнкодер, который обучают так, чтобы его латентное пространство стало **упорядоченным и пригодным для генерации из случайного шума**.

Обычный AE учится: $$x \rightarrow z \rightarrow \hat{x}$$ и минимизирует только ошибку восстановления.

VAE делает почти то же, но с ключевыми отличиями ниже.

***

### Чем VAE отличается от обычного AutoEncoder

#### 1) Encoder выдаёт не один вектор $$z$$, а распределение

В обычном AE:

* encoder даёт конкретный код $$z = E(x)$$

В VAE:

* encoder даёт параметры распределения (чаще всего нормального): $$\mu(x),; \log\sigma^2(x)$$ и считается, что $$q(z|x) = \mathcal{N}(\mu(x), \mathrm{diag}(\sigma^2(x)))$$

То есть для одного и того же $$x$$ можно получить разные $$z$$ (через сэмплирование).

#### 2) В loss добавляется регуляризация (KL-дивергенция)

VAE оптимизирует сумму двух частей:

1. **Reconstruction loss** (как в AE): чтобы $$\hat{x}$$ был похож на $$x$$
2. **KL-штраф**: чтобы распределение $$q(z|x)$$ было похоже на заранее выбранный prior $$p(z)$$

Обычно берут: $$p(z) = \mathcal{N}(0, I)$$

И итог: $$L = L_{recon}(x,\hat{x}) + KL(q(z|x)|p(z))$$

Смысл KL-штрафа:

* заставляет коды не “разлетаться” куда угодно
* делает латентное пространство “плотным” и “ровным”
* чтобы можно было брать случайный $$z \sim \mathcal{N}(0, I)$$ и получать нормальные картинки

#### 3) VAE — вероятностная модель

У него смысл: мы хотим моделировать распределение данных $$p(x)$$ через скрытые переменные $$z$$. Поэтому важны распределения, а не один детерминированный код.

***

### Как VAE генерирует латентный вектор $$z$$

Есть два режима.

#### A) Во время обучения (из конкретного $$x$$)

1. Encoder считает $$\mu(x)$$ и $$\sigma(x)$$
2. Потом берём случайный шум: $$\epsilon \sim \mathcal{N}(0, I)$$
3. Получаем латентный вектор через reparameterization trick: $$z = \mu + \sigma \odot \epsilon$$

Здесь $$\odot$$ — поэлементное умножение.

Зачем это нужно: чтобы можно было делать backprop через случайность. Случайность “вынесли” в $$\epsilon$$, а $$\mu,\sigma$$ остаются дифференцируемыми.

#### B) При генерации новых картинок “с нуля”

Тут входа $$x$$ нет. Мы просто:

1. Сэмплируем: $$z \sim \mathcal{N}(0, I)$$
2. Декодируем: $$x_{gen} = D(z)$$

Это и есть “правильная генерация” в отличие от обычного AE: мы заранее знаем, откуда брать $$z$$.

***

### Что сказать на защите одним блоком

* AE: детерминированный код $$z$$, минимизирует только reconstruction error, случайный $$z$$ обычно не работает.
* VAE: код — распределение $$q(z|x)$$, loss = reconstruction + KL к $$\mathcal{N}(0,I)$$, поэтому можно генерировать, делая $$z \sim \mathcal{N}(0,I)$$.
