# Вопрос №5.09

> Архитектура ResNet (Residual Network)

#### Идея

ResNet строится из **остаточных (residual) блоков**. Вместо того чтобы учить прямое преобразование $$H(x)$$, блок учит “поправку” $$F(x)$$ к входу $$x$$:

$$y = F(x) + x$$

Где:

* $$x$$ — вход блока (тензор признаков),
* $$F(x)$$ — результат нескольких слоёв (обычно свёртки + BatchNorm + ReLU),
* $$y$$ — выход блока.

Эта добавка $$+x$$ называется **skip connection** (shortcut).

#### Как устроен residual block

Есть два типичных варианта:

**1) BasicBlock (ResNet-18/34):**

* $$3 \times 3$$ conv → BN → ReLU
* $$3 \times 3$$ conv → BN
*
  * shortcut, затем ReLU

**2) Bottleneck (ResNet-50/101/152):**

* $$1 \times 1$$ conv (сжать каналы) → BN → ReLU
* $$3 \times 3$$ conv → BN → ReLU
* $$1 \times 1$$ conv (расширить каналы) → BN
*
  * shortcut, затем ReLU Обычно расширение каналов в bottleneck: $$C_{out} = 4 \cdot C_{mid}$$.

Если размеры не совпадают (например, меняется число каналов или stride), shortcut делают через $$1 \times 1$$ conv: $$x' = W_s x,\quad y = F(x) + x'$$

***

### Как ResNet улучшает поток градиента

Проблема глубоких сетей: при backprop градиенты могут **затухать** (vanishing gradients), и ранние слои учатся плохо.

В residual блоке: $$y = F(x) + x$$

Градиент по входу: $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}\left(1 + \frac{\partial F}{\partial x}\right)$$

Ключевой момент — слагаемое $$1$$. Даже если $$\frac{\partial F}{\partial x}$$ маленькое или “шумное”, остаётся прямой путь передачи градиента: $$\frac{\partial L}{\partial x} \approx \frac{\partial L}{\partial y}$$

Интуитивно: сеть всегда может “пропустить” информацию дальше почти без изменений (identity mapping), поэтому:

* легче обучать очень глубокие модели,
* оптимизация стабильнее,
* ранние слои получают нормальный сигнал обучения.

***

### Как использовать ResNet для transfer learning

#### 1) Вариант “feature extractor” (быстро и надёжно)

1. Берём ResNet, предобученную на ImageNet.
2. Убираем/заменяем последний классификатор (FC-слой) под свои классы.
3. **Замораживаем** все свёрточные слои (их веса не обновляются).
4. Обучаем только новую “голову”.

Когда подходит: мало данных, нужно быстро получить хороший базовый результат.

#### 2) Вариант “fine-tuning” (лучше качество, но аккуратно)

1. Сначала обучаем новую голову при замороженной базе.
2. Потом **размораживаем** верхние блоки (например, последний stage) и дообучаем.
3. Используем меньший learning rate для базы, больший — для головы.

Когда подходит: данных больше, домен отличается от ImageNet (медицина, специфичные объекты).

#### Что обычно меняют в ResNet при transfer learning

* **Последний FC-слой**: вместо $$1000$$ классов на ImageNet ставим $$K$$ классов задачи.
* Иногда добавляют Dropout в голову (если переобучение).
* Если вход не RGB (например 1 канал), либо дублируют канал до 3, либо меняют первый conv.

#### Эмбеддинг из ResNet

Часто ResNet используют как “экстрактор эмбеддинга”:

* берут выход после global average pooling (до FC),
* получают вектор (например, $$512$$ для ResNet-18/34, $$2048$$ для ResNet-50+),
* дальше используют для поиска похожих, кластеризации, простой классификации поверх эмбеддингов.
