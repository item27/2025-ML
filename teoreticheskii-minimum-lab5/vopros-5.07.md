# Вопрос №5.07

> batchnorm2d. Как работает, обучается ли слой, зачем нужен, какие гиперпараметры и на что влияют

### BatchNorm2D: что это и где применяется

BatchNorm2D (Batch Normalization для свёрточных сетей) — слой, который **нормализует активации по батчу**, отдельно для каждого канала (карты признаков), и затем делает **обучаемое масштабирование и сдвиг**.

Вход имеет форму $$N \times C \times H \times W$$ (батч, каналы, высота, ширина).

***

### Как работает (формулы)

Для каждого канала $$c$$ считаем статистики по всем элементам батча и по всем пикселям карты:

Среднее:

$$
\mu_c = \frac{1}{N H W}\sum_{n,h,w} x_{n,c,h,w}
$$

Дисперсия:

$$
\sigma_c^2 = \frac{1}{N H W}\sum_{n,h,w} (x_{n,c,h,w} - \mu_c)^2
$$

Нормализация:

$$
\hat{x}*{n,c,h,w}=\frac{x*{n,c,h,w}-\mu_c}{\sqrt{\sigma_c^2+\varepsilon}}
$$

Дальше обучаемое преобразование:

$$
y_{n,c,h,w} = \gamma_c \hat{x}_{n,c,h,w} + \beta_c
$$

Здесь:

* $$\gamma_c$$ (scale) и $$\beta_c$$ (shift) — **обучаемые параметры** (по одному на канал),
* $$\varepsilon$$ — маленькое число для стабильности.

***

### Обучается ли BatchNorm2D

Да, но частично:

1. **Обучаемые параметры:** $$\gamma_c$$ и $$\beta_c$$ обучаются градиентным спуском, как обычные веса.
2. **Статистики:** среднее и дисперсия в train-режиме считаются из текущего батча, а для inference (eval) используются **running mean/var** — скользящие оценки, которые обновляются во время обучения.

***

### Зачем нужен BatchNorm2D

1. **Стабилизирует обучение**: распределения активаций не “уплывают” слишком сильно между итерациями.
2. **Позволяет брать больший learning rate** и быстрее сходиться.
3. **Уменьшает чувствительность к инициализации**.
4. Часто даёт **лёгкий регуляризирующий эффект** (из-за шума статистик батча).

Практически: с BN обычно легче обучать глубокие CNN и получать лучше метрики.

***

### Гиперпараметры и на что влияют

| Гиперпараметр         | Что это                              | Влияние                                                                                |
| --------------------- | ------------------------------------ | -------------------------------------------------------------------------------------- |
| $$\varepsilon$$ (eps) | добавка в корне                      | численная стабильность; слишком маленькое может давать проблемы                        |
| momentum              | скорость обновления running mean/var | больше momentum → running статистики быстрее подстраиваются; влияет на качество в eval |
| affine (True/False)   | использовать ли $$\gamma,\beta$$     | если False — нет обучаемого масштаба/сдвига (редко нужно)                              |
| track\_running\_stats | хранить ли running mean/var          | если False — в eval будут использоваться статистики батча (обычно хуже/нестабильно)    |

***

### Важные практические моменты

* **Train vs Eval:** поведение разное. В train берём статистики батча, в eval — running.
* **Маленький batch size**: BN может работать хуже, потому что оценки $$\mu_c, \sigma_c^2$$ шумные. Тогда часто используют GroupNorm/LayerNorm или “замораживают” BN.
* Часто **bias у conv отключают**, если сразу идёт BatchNorm, потому что $$\beta$$ делает сдвиг.

