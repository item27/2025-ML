# Вопрос №5.02

> Что такое свёрточный слой в глубокому обучении

Свёрточный слой (Convolutional layer) — это слой нейросети, который превращает входную “картинку” (тензор) в набор **карт признаков** (feature maps), применяя **набор обучаемых фильтров** (ядер) к локальным областям входа.

Если вход — изображение $$H \times W \times C$$ (высота, ширина, каналы), то каждый фильтр имеет размер $$K_h \times K_w \times C$$ и “скользит” по картинке. В каждой позиции считается скаляр:

$$
y(i,j) = \sum_{u,v,c} x(i+u, j+v, c), w(u,v,c) + b
$$

Один фильтр даёт одну карту признаков. $$N$$ фильтров дают выход $$H' \times W' \times N$$.

Важно: в DL чаще реализуют **кросс-корреляцию**, но называют “свёрткой”; смысл для CNN тот же: локальная взвешенная сумма.

***

### Гиперпараметры свёрточного слоя и влияние

| Гиперпараметр                         | Что это                                | На что влияет                                                                         |
| ------------------------------------- | -------------------------------------- | ------------------------------------------------------------------------------------- |
| Кол-во фильтров $$N$$ (out\_channels) | сколько ядер                           | сколько разных признаков слой может выучить; ширина модели; скорость/память           |
| Размер ядра $$K_h \times K_w$$        | размер “окна”                          | размер локального контекста; большие ядра видят больше, но дороже и больше параметров |
| Stride $$S$$                          | шаг сдвига                             | уменьшает $$H',W'$$ (downsampling); больше stride → меньше деталей, быстрее           |
| Padding $$P$$                         | дополнение по краям                    | сохраняет размер (например, “same”); без padding теряются границы                     |
| Dilation $$D$$                        | “разреженность” ядра                   | увеличивает эффективное поле зрения без роста параметров                              |
| Groups                                | разбиение каналов на группы            | экономия вычислений; special-case: depthwise conv                                     |
| Bias (да/нет)                         | смещение $$b$$                         | чуть повышает гибкость (часто отключают при BatchNorm)                                |
| Activation (ReLU и др.)               | нелинейность после conv                | позволяет учить сложные функции; без неё сеть почти линейная                          |
| BatchNorm/Dropout рядом               | не гиперпараметр conv, но часто вместе | стабилизация обучения / регуляризация                                                 |

**Размер выхода** (для одного измерения) часто считают так:

$$
H' = \left\lfloor \frac{H + 2P - D\cdot (K_h-1) - 1}{S} + 1 \right\rfloor
$$

аналогично для $$W'$$.

***

### Как свёрточный слой обучается

В слое есть параметры: веса фильтров $$w$$ и смещения $$b$$.

1. **Forward:** слой применяет фильтры → получает карты признаков.
2. **Loss:** считаем ошибку (например, кросс-энтропию).
3. **Backprop:** автоматически считаются градиенты $$\frac{\partial L}{\partial w}$$ и $$\frac{\partial L}{\partial b}$$.
4.  **Update:** оптимизатор (SGD/Adam) обновляет параметры:

    $$
    w \leftarrow w - \eta \frac{\partial L}{\partial w}
    $$

Слой ничего “вручную” не ищет — он **подбирает фильтры**, которые уменьшают ошибку на задаче.

***

### Какие признаки извлекаются на разных слоях CNN

Общее правило: чем глубже слой, тем **более абстрактные** признаки (и больше рецептивное поле — область входа, влияющая на нейрон).

#### Ранние слои (первые 1–2 conv)

**Что видят:** очень локально. **Что учат:**

* границы/контуры (edge detectors)
* простые направления линий
* точки, углы
* простые текстуры

Пример: фильтр похож на “Собеля” для выделения вертикальных/горизонтальных границ.

#### Средние слои

**Что видят:** уже куски объекта. **Что учат:**

* комбинации границ → простые формы
* повторяющиеся текстуры (полосы, клетки)
* “части” объектов (дуги, элементы одежды, куски цифры)

#### Глубокие слои (ближе к выходу)

**Что видят:** крупные области, почти целый объект. **Что учат:**

* высокоуровневые паттерны: “похоже на цифру 8”, “есть подошва и шнурки”
* композиции частей в целое
* признаки, полезные именно для классов датасета

***

### Почему это работает именно для изображений

Ключевые свойства conv-слоя:

1. **Локальность:** признаки ищутся в маленьких окнах.
2. **Разделение весов:** один и тот же фильтр используется во всех местах изображения → меньше параметров.
3. **Эквивариантность к сдвигу:** если объект сместился, карта признаков сместится похожим образом.
4. **Иерархия:** слои строят “от простого к сложному”.

