# Вопрос №3.11

> Что такое `approximated nearest neighbors (ann)` search?\
> Как использовать `kd-tree` для реализации этого алгоритма?\
> Какое время работы у `ann`?

ANN — это задача поиска ближайших соседей с допусками в точности ради ускорения. Вместо точного NN ищут приближённый сосед $$x'$$, который удовлетворяет

$$
\operatorname{dist}(q,x') \le (1+\varepsilon),\operatorname{dist}(q,x^*)
$$

где $$x^*$$ — точный ближайший сосед, $$\varepsilon\ge0$$ — допустимая погрешность. Это даёт большой выигрыш по времени при почти равном качестве результатов.

## Зачем нужен ANN

* Точное NN в больших наборах и при высокой размерности дорого (наивно $$O(n)$$ на запрос).
* Для многих приложений (рекомендации, поиск похожих картинок, кластеризация, индексирование) достаточно приближённого ответа и важна скорость.

## Основные подходы к ANN (кратко)

* Пространственные деревья: kd-tree, ball-tree, cover-tree.
* Графовые структуры: HNSW (hierarchical navigable small world).
* Проективные/деревья случайных проекций: Annoy, random projection trees.
* LSH (Locality Sensitive Hashing) — хеширование с вероятностным гарантом.
* Библиотеки: FLANN, Faiss, Annoy, hnswlib.

## Как использовать kd-tree для ANN

kd-tree — бинарное разбиение по осям. Для точного NN стандартный алгоритм рекурсивный: спуск по дереву к листу, затем «backtrack» и проверка других ветвей, если гиперпрямоугольник может содержать ближе точку.

Для приближённого поиска делают упрощения, чтобы сократить число посещённых листьев:

1. Построение:
   * Разбиение по медиане по выбранной координате (или по наибольшему разбросу).
   * Временная сложность сборки: $$O(n\log n)$$ (обычно).
2. Поиск с ранней остановкой (Best-Bin-First / ограничение проверок):
   * Использовать приоритетную очередь узлов по расстоянию от запроса до их bounding-box.
   * Вытаскивать узлы с наименьшим приоритетом и исследовать их.
   * Остановить поиск после рассмотрения не более $$C$$ листов (параметр `checks` / `max_visits`) или после нахождения соседей с расстоянием меньше заданного порога.
   * Чем меньше $$C$$, тем быстрее и менее точный результат. Когда $$C$$ достаточно велик, поведение стремится к точному NN.

Псевдокод (иллюстративно):

```
push root into PQ with priority = dist_to_box(root)
visited_leaves = 0
while PQ not empty and visited_leaves < C:
    node = PQ.pop()
    if node is leaf:
        check all points in leaf, update best
        visited_leaves += 1
    else:
        push child_left with priority dist_to_box(left)
        push child_right with priority dist_to_box(right)
return best
```

3. Множественные деревья и рандомизация:
   * Построить несколько kd-деревьев с разными случайными сдвигами/осями.
   * Искать по всем деревьям с малым $$C$$ на каждый. Это повышает вероятность найти хороший сосед при том же времени.
4. Параметры, которые практически важны:
   * `leaf_size` — сколько точек хранить в листе; больше ускоряет поиск за счёт точек в листе.
   * `max_checks` / `search_k` / `max_visits` — сколько листьев/вершин проверять.
   * количество деревьев (если используется ансамбль).
   * допустимая погрешность $$\varepsilon$$ (иногда задаётся через `approx_factor`).

## Время работы (оценки)

* Построение kd-tree: обычно $$O(n\log n)$$.
* Точный запрос в низкой размерности (усреднённо): $$O(\log n)$$ при сбалансированном дереве и малом $$d$$.
* Точный запрос — худший случай: $$O(n)$$ (особенно в высокой размерности или при плохом разбиении).
* Приближённый поиск (ограничение просмотров) — ожидаемо сублинейный, часто близок к $$O(\log n)$$ или меньше на практике. Скорость зависит от параметра `C`.
* Важно: эффективность kd-tree сильно деградирует с ростом размерности $$d$$ (куча точек в листьях, много ветвей нужно проверять). При больших $$d$$ kd-tree часто не даёт ускорения по сравнению с наивным перебором.

Для других современных ANN-структур:

* HNSW: построение \~$$O(n\log n)$$ (практически), поиск \~$$O(\log n)$$ в среднем; на практике очень быстрый и устойчивый при больших $$d$$.
* LSH: построение и поиск зависят от числа хешей; типично поиск подлинейный $$O(n^\rho)$$ с $$\rho<1$$ зависящим от сигнатуры хешей. (Эти оценки эмпирические и зависят от реализации и данных.)

## Практические замечания и советы

* Если $$d$$ малое (например $$d\le 10$$), kd-tree (точный) часто лучше.
* Для средних $$d$$ (10–50) kd-tree с approximate-search (ограничение просмотров) может подойти.
* Для больших $$d$$ (>50–100) используйте HNSW / Faiss / Annoy / LSH.
* Часто предварительно уменьшают размерность (PCA 30–100 компонент) перед индексированием ANN. Это сохраняет структуру и ускоряет поиск.
* Тестируйте качество (recall@k) против времени: настраивайте `checks`/`search_k` чтобы получить требуемый recall при минимальном времени.

## Короткая шпаргалка для сдачи

* ANN ≈ быстрый NN с гарантией $$\operatorname{dist}\le(1+\varepsilon),d^*$$.
* kd-tree для ANN = тот же kd-tree + ранняя остановка / best-bin-first / несколько деревьев.
* Построение $$O(n\log n)$$. Точный NN: средне $$O(\log n)$$, худшее $$O(n)$$. ANN: обычно сублинейно, скорость настроена параметром числа просканированных листьев.
* В высокой размерности используйте современные методы (HNSW, Faiss).
