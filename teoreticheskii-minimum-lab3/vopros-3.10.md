# Вопрос №3.10

> Как использовать алгоритмы обучения без учителя для поиска выбросов

Коротко: методы без учителя для поиска выбросов определяют аномалии как точки, которые плохо вписываются в общую структуру данных. Выбор метода зависит от формы данных, размерности и скорости работы. Ниже — компактный обзор методов, когда их применять, как настроить и как оценивать результаты.

## Основные подходы и алгоритмы

1. **Одномерные методы (простые случаи)**
   * Z-score: пометить как аномалию, если $$|z|>t$$, где $$z=(x-\mu)/\sigma$$.
   * Modified Z (по медиане, устойчивее к выбросам): используют медиану и MAD. Применять для отдельных признаков.
2. **Мультимодальные и многомерные геометрические**
   *   **Mahalanobis distance**. Для предполагаемой гауссовости:

       $$
       D_M(x)=\sqrt{(x-\mu)^\top S^{-1}(x-\mu)}.
       $$

       Порог можно выбрать по распределению $\chi^2$ (степени свободы = число признаков). Хорошо, если данные примерно нормальны и мало размерностей. Требует оценивания ковариации устойчиво (robust covariance).
3. **Плотностные методы**
   * **LOF (Local Outlier Factor)** — сравнивает локальную плотность точки и соседей. Выявляет локальные аномалии (подходит при изменяющейся плотности). Чувствителен к выбору $$k$$.
   * **DBSCAN** — точки, отмеченные как шум, могут быть выбросами, если кластеры плотны и выбросы разрежены.
4. **Методы на основе расстояний**
   * **k-NN distance**: расстояние до k-го соседа как оценка аномалии. Простая и понятная. Порог — по к-dist plot или по квантилю.
5. **Ансамблевые и деревоподобные**
   * **Isolation Forest** — случайное разбиение пространства. Быстро, масштабируемо, часто хороший выбор по умолчанию. Параметр `contamination` задаёт ожидаемую долю выбросов. Хорош для больших наборов и высокой размерности.
6. **Граничные / ядровые**
   * **One-Class SVM** — строит границу области плотности. Работает при малых выборках, чувствителен к масштабированию и параметрам (`nu`, `kernel`). Вычислительно дороже.
7. **Реконструкционные / нейросетевые**
   * **Автоэнкодеры**: обучить кодировщик-декодировщик и считать большие ошибки восстановления аномалиями. Подходит для высокой размерности и нелинейных зависимостей. Нужны данные и настройка архитектуры.
   * **PCA-реконструкция**: использовать ошибку восстановления из первых $$r$$ компонент как меру аномалии.
8. **Кластеризационные**
   * Кластеризовать (k-means, GMM) и считать точки маленьких кластеров или тех, у которых большое расстояние до центра, аномалиями.

## Практический пайплайн (рекомендуемый)

1. **Предобработка**: очистка дубликатов, обработка пропусков, кодирование категорий.
2. **Масштабирование**: StandardScaler или RobustScaler (robust при выбросах). Масштаб важен для всех методов на базе расстояний.
3. **Снижение размерности** (по необходимости): PCA до 10–50 компонент если d велико, чтобы убрать шум и ускорить алгоритмы плотности.
4. **Выбор алгоритма** по задачам (см. ниже).
5. **Тюнинг порога**: либо параметр `contamination`, либо порог на score (q-квантиль на валидации) либо статистический порог (χ² для Mahalanobis).
6. **Валидация**: если есть метки — обычная CV; если нет — использовать экспертную разметку подвыборки, визуализацию, проверку на временных отрезках.
7. **Постобработка**: агрегация аномалий во временные окна, фильтрация ложных срабатываний, объяснение (feature attribution).

## Как выбирать метод (heuristics)

* Небольшая размерность, предполагаемая гауссовость → Mahalanobis или EllipticEnvelope.
* Высокая размерность, большие данные → Isolation Forest (по умолчанию).
* Локальные аномалии и переменная плотность → LOF или k-NN distance.
* Нелинейные сложные зависимости → автоэнкодеры или PCA+OneClassSVM.
* Простая, быстая проверка → k-NN distance или IsolationForest.
* Если нужны «шумовые» точки при кластеризации → DBSCAN (точки, помеченные шумом).

## Настройка и параметры (практические советы)

* **Scale** перед применением. RobustScaler при наличии сильных выбросов.
* Для LOF и kNN выбирать $$k$$ порядка $$\sqrt{n}$$ как грубое начало или пробовать 10–50.
* Для IsolationForest задавать `n_estimators` (100−200), `max_samples` (auto или дробь), `random_state`. Параметр `contamination` важен для порогов.
* Для автоэнкодера: нормализовать, регуляризация (dropout, weight decay), использовать validation loss для порога.
* Для PCA-реконструкции выбирать число компонент по доле объяснённой дисперсии или через кросс-валидацию для downstream задачи.

## Оценка качества (без меток)

* Визуализация (PCA/TSNE/UMAP) и ручная проверка найденных точек.
* Синтетическая инжекция аномалий (выбрасывать N случайных точек/образовать аномалии) для тестирования детектора.
* Метрики при наличии меток: precision, recall, F1, PR-AUC (лучше ROC при сильном дисбалансе).
* Скорость и пропускная способность важны для продакшн-решений.

## Примеры порогов

* Mahalanobis: использовать квантиль распределения $\chi^2\_d$ (например, 0.975).
* IsolationForest: `contamination=0.01` для 1% ожидаемых выбросов.
* kNN distance: порог — верхний квантиль (например, 95-й) распределения k-dist на обучающей выборке.

## Масштабируемость и сложность

* LOF / kNN (наивно) — $$O(n^2)$$. KD-tree/ball-tree ускоряют в низкой d.
* IsolationForest — примерно $$O(n \log n)$$ в среднем (реализации зависят). Подходит для больших n.
* One-Class SVM — дорогой, не для больших n.
* Автоэнкодеры — затратны по обучению, но затем быстры в инференсе.

## Интерпретируемость

* Для деревьев/IsolationForest можно получить важности признаков.
* Для реконструкционных методов смотреть вклад ошибки в каждой переменной.
* Для объяснений применять SHAP/feature-attribution.

## Резюме (коротко)

* **IsolationForest** и **LOF/kNN** — самые распространённые выборы без меток.
* Масштабируйте данные, при высокой размерности подумайте о PCA.
* Подбирайте порог через `contamination` или валидацию.
* Оценивайте результаты визуально и/или через синтетическую инъекцию аномалий.
* Для временных рядов используйте специализированные методы (скользящие окна, сезонная декомпозиция, LSTM/Autoencoder для последовательностей).
