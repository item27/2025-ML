# Вопрос №3.08

> Модель `pca`

PCA (Principal Component Analysis) — линейный метод снижения размерности, который ищет ортонормированные направляния (компоненты), максимизирующие дисперсию данных. Часто используют для уменьшения шума, визуализации и ускорения обучения.

## Цель (две эквивалентные формулировки)

1.  Найти матрицу проекции $$W_{d\times r}$$ (столбцы — ортонормированные векторы), максимизирующую суммарную дисперсию проекций:

    $$
    \max_{W^\top W = I} \operatorname{tr}(W^\top S W),
    $$

    где $$S$$ — ковариационная матрица центровных данных.
2.  Минимизировать ошибку восстановления при проекции на подпространство ранга $$r$$:

    $$
    \min_{W}|X - X W W^\top|_F^2.
    $$

## Как получить компоненты (алгоритм)

1. Центрировать данные: $$X_c = X - \bar X$$.
2.  Построить ковариационную матрицу:

    $$
    S = \frac{1}{n} X_c^\top X_c.
    $$
3.  Найти собственные векторы и собственные значения:

    $$
    S v_i = \lambda_i v_i,\qquad \lambda_1 \ge \lambda_2 \ge \dots
    $$
4.  Матрица проекции на первые $$r$$ компонент:

    $$
    W = [v_1,\dots,v_r].
    $$

    Проекция объектов:

    $$
    Z = X_c W.
    $$

Альтернативный численно устойчивый путь — SVD:

$$
X_c = U\Sigma V^\top,\quad W = V_{[:,:r]},\quad Z = U_{[:,:r]}\Sigma_{[:r,:r]}.
$$

## Объяснённая дисперсия и выбор числа компонент

Собственное значение $$\lambda_i$$ равно дисперсии вдоль $$i$$-й компоненты. Доля объяснённой дисперсии для первых $$r$$ компонент:

$$
\frac{\sum_{i=1}^r \lambda_i}{\sum_{i=1}^d \lambda_i}.
$$

Часто выбирают $$r$$ так, чтобы доля достигала порога (например, $$90%-95%$$) или по «колену» на графике объяснённой дисперсии.

## Восстановление и ошибка

Восстановление из первых $$r$$ компонент:

$$
\hat X = Z W^\top + \bar X.
$$

Ошибка восстановления (фробениусова норма):

$$
|X - \hat X|*F^2 = \sum*{i=r+1}^d \lambda_i.
$$

## Whitening

Приведение компонент к единичной дисперсии:

$$
Z_{\text{white}} = Z \Lambda^{-1/2},
$$

где $$\Lambda=\operatorname{diag}(\lambda_1,\dots,\lambda_r)$$. В sklearn: `PCA(whiten=True)`.

## Свойства, плюсы и минусы

* Линейный метод. Не захватывает нелинейные структуры.
* Максимизирует дисперсию, а не разделимость классов.
* Чувствителен к масштабу признаков. Центрировать обязательно. Часто стандартизировать.
* Устойчив к коррелированности признаков: PCA удаляет коллинеарность.
* Быстро уменьшает размерность при большой избыточности признаков.
* Чувствителен к выбросам (они сильно влияют на собственные значения).

## Сложность

Полная SVD: примерно $$O(\min(nd^2, n^2 d))$$. Для больших данных используют randomized SVD или incremental PCA.

## Практические рекомендации

* Всегда центрировать данные перед PCA.
* Если признаки в разных единицах — стандартизировать (StandardScaler).
* Для обучения моделей: пробовать оба варианта — исходные признаки и PCA-проекцию. PCA полезен для линейных моделей и алгоритмов, чувствительных к размерности. Для деревьев/градиентного бустинга PCA часто не нужен и может ухудшить интерпретируемость.
* Для визуализации брать первые 2–3 компоненты (t-SNE/UMAP лучше для нелинейной структуре).
* Выбирать число компонент по кросс-валидации или по доле объяснённой дисперсии.
* Не выполнять `fit` PCA на всей выборке, если потом тестируете — `fit` только на train, затем `transform` на val/test.

## Быстрый пример (sklearn)

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline

pipe = make_pipeline(StandardScaler(), PCA(n_components=0.95))  # сохраняет 95% дисперсии
X_train_reduced = pipe.fit_transform(X_train)
X_val_reduced   = pipe.transform(X_val)
```

## Короткая шпаргалка для сдачи

* PCA = собственные векторы ковариации / SVD.
* Проекция: $$Z = X_c W$$.
* Объяснённая дисперсия = сумма первых собственных значений / сумма всех.
* Центрировать обязательно. Масштабировать при разных единицах.
* Линейный, максимум дисперсии, не обязательно максимум разделимости.
