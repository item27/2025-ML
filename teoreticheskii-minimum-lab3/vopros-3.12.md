# Вопрос №3.12

> Как автоматически определить количество кластеров в данных?

Можно. Есть несколько проверенных автоматических подходов. Обычно берут набор методов, смотрят их согласованность и выбирают стабильное значение $$k$$. Ниже — кратко по методам, формулам, практическому пайплайну и замечаниям.

## Методы и формулы (суть)

* **Elbow (WSS / внутри-кластерная сумма квадратов)** $$WSS(k)=\sum_{i=1}^k\sum_{x\in C_i}|x-\mu_i|^2.$$ Строят график $$WSS(k)$$ и ищут «локоть» (knee). Автоматически можно детектировать локоть алгоритмом Kneedle.
* **Силуэт (silhouette)** Для точки $$i$$: $$s(i)=\dfrac{b(i)-a(i)}{\max(a(i),b(i))}$$. Берут среднее $$\bar s(k)$$ и выбирают $$k$$, максимизирующее $$\bar s$$.
* **Gap statistic** (Tibshirani et al.) $$\text{gap}(k)=\mathbb{E}*{\text{ref}}[\log W_k^{*}]-\log W_k,$$ где $$W_k$$ — WSS для данных, $$W_k^{*}$$ — для случайных референсных наборов. Выбирают первый $$k$$ такой, что $$\text{gap}(k)\ge \text{gap}(k+1)-s*{k+1}$$ (с учётом погрешности).
* **Calinski–Harabasz (CH)** $$CH(k)=\frac{BSS/(k-1)}{WSS/(n-k)},$$ где $$BSS$$ — между-кластерная дисперсия. Чем больше — лучше. Выбирать $$k$$ с максимальным $$CH$$.
* **Davies–Bouldin (DB)** $$DB=\frac{1}{k}\sum_{i=1}^k \max_{j\ne i}\frac{s_i+s_j}{d(\mu_i,\mu_j)},$$ где $$s_i$$ — средний внутрикластерный разброс. Чем меньше $$DB$$ — лучше.
* **Model-based (GMM + BIC / AIC)** Обучают GMM с разным $$k$$ и выбирают $$k$$ минимизирующий $$BIC$$ или $$AIC$$: $$BIC = -2\log L + p\log n.$$
* **Spectral / eigen-gap** Для графа смежности/лапласиана берут спектр $$\lambda_1\le\lambda_2\le\dots$$ и ищут большой разрыв между $$\lambda_k$$ и $$\lambda_{k+1}$$ (eigen-gap).
* **Stability / consensus (bootstrap)** Повторно кластеризуют подвыборки. Оценивают стабильность меток (ARI, NMI). Выбирают $$k$$ с максимальной стабильностью.
* **Density-based альтернативы** Если кластеры разной формы/плотности — не выбирать $$k$$, а использовать DBSCAN/HDBSCAN; HDBSCAN сам выделяет количество кластеров.

## Практический пайплайн (рекомендация)

1. Предобработка: очистить, масштабировать, при необходимости снизить размерность (PCA → 10–50 комп.).
2. Задать разумный диапазон $$k$$, например $$k\in[2,,\min(20,\sqrt{n})]$$ или $$[2,20]$$.
3. Для каждого $$k$$ вычислить: $$WSS$$ (elbow), $$\bar s$$ (silhouette), $$CH$$, $$DB$$, gap statistic и (по желанию) BIC для GMM.
4. Визуализировать метрики и искать согласие:
   * Elbow + Kneedle для локтя.
   * Максимум $$\bar s$$ и $$CH$$, минимум $$DB$$.
   * Gap: выбрать первый $$k$$ по правилу с погрешностью.
   * BIC для GMM как доп. проверка (если данные близки к гауссовым).
5. Проверить стабильность через бутстрэп (ARI между запусками).
6. Если методы расходятся, предпочесть тот, который согласуется с предметной задачей и визуализацией (PCA/UMAP + цвет по кластерам).
7. Если форма кластеров сложная — попробовать HDBSCAN/DBSCAN вместо фиксированного $$k$$.

## Практические заметки и правила выбора

* **Силуэт** хорошо работает при компактных, равномерных кластерах.
* **Gap** более статистически обоснован, но дороже по вычислениям (генерация референсных наборов).
* **BIC/GMM** подходят, если кластеры близки к гауссам.
* **Spectral eigen-gap** полезен для кластеров, хорошо представленных графом (связность).
* В **высокой размерности** сначала делать PCA; многие индексы страдают из-за концентрации расстояний.
* Часто оптимальная стратегия — совокупность: elbow + silhouette + gap + stability.
* Для больших $$n$$ gap и stability дорогие; используют подвыборки.

## Автоматизация (быстро)

* Автоматическое правило: выбрать $$k$$, дающий максимум согласующихся индексов (например, голосование CH+silhouette+gap).
* Или: выбрать $$k$$ с максимальной stability (ARI) при бутстрэпе.
* Для быстрых систем: пробовать GMM BIC и silhouette, выбрать $k$ если оба согласны; иначе интерактивная проверка.

## Сложность и ресурсы

* WSS/silhouette/CH/DB — $$O(n^2)$$ для силуэта (наивно), $$O(nkd)$$ для k-means оценки WSS. Gap требует генерации $B$ референсов → умножает время. Stability потребует множества запусков.
* Для больших данных использовать подвыборки или приближённые методы.

## Короткая шпаргалка для сдачи

* Попробовать одновременно: elbow (WSS), silhouette, gap, CH, DB, BIC-GMM.
* Выбирать $$k$$ при согласием нескольких метрик или по стабильности кластеров.
* Для сложных форм не искать $$k$$ — использовать density-based (HDBSCAN).
