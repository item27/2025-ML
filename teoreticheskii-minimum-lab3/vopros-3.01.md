# Вопрос №3.01

> Что такое проклятие размерности

Проклятие размерности — это явление, при котором поведение данных и эффективность алгоритмов заметно ухудшаются по мере роста числа признаков (размерности пространства).

## Что это значит простыми словами

* В многомерном пространстве точки становятся «редкими». При тех же объёмах выборки расстояния между точками увеличиваются и перестают быть информативными.
* Интуитивно: если добавить много независимых признаков, объём возможных состояний растёт экспоненциально, и наблюдений уже не хватает, чтобы покрыть пространство.
* Для алгоритмов, опирающихся на расстояния или плотности, это значит, что «близкие» и «далёкие» соседи перестают отличаться.

## Почему это происходит (коротко, с интуицией)

* Объём растёт экспоненциально с размерностью. Чтобы заполнить d-мерный куб той же плотностью, нужно экспоненциально больше точек.
* Концентрация расстояний: в высоких размерностях разброс расстояний между любыми двумя точками становится мал; отношение (min\_dist / max\_dist) стремится к 1. Это ломает методы, которые выбирают ближайших соседей.
* Шум и малозаметные признаки: много признаков добавляют шум, модель подстраивается под шум (переобучение).

## Какие проблемы вызывает на практике

* k-NN, k-means, Density-based методы теряют качество, потому что расстояния теряют смысл.
* Оценки плотности и статистические методы требуют больше данных.
* Модели чаще переобучаются при фиксированном размере выборки.
* Интерпретируемость падает.

## Простая математическая ремарка

Если нужно захватить долю объёма (p) в единичном кубе в каждой оси радиусом (r), то (r \approx p^{1/d}). При росте (d) значение (p^{1/d}) быстро стремится к 1. То есть «локальная» окрестность должна сильно расширяться, чтобы содержать ту же долю объёма.

## Как с этим бороться (практические приёмы)

1. Уменьшение размерности. PCA, автоэнкодеры, UMAP/t-SNE для визуализации.
2. Отбор признаков. Удалять коррелированные и нерелевантные признаки.
3. Масштабирование признаков. Нормализация и стандартизация уменьшают доминирование признаков с большим размахом.
4. Регуляризация моделей. L1/L2, отсечение признаков.
5. Использовать методы, нечувствительные к евклидовым расстояниям: косинусное сходство, деревья решений (хотя и они страдают при очень высокой размерности).
6. Применять приближённые структуры для поиска соседей (ANN) вместо точных, если критична скорость.
7. Увеличивать объём данных, но это часто нецелесообразно из-за экспоненциального роста требований.

## Что запомнить к сдаче (коротко)

* Суть: при росте числа признаков данные становятся разреженными, расстояния концентрируются, алгоритмы на расстояниях теряют смысл.
* Последствия: ухудшение качества, необходимость экспоненциально больше данных, риск переобучения.
* Решения: снижать размерность, отбирать признаки, масштабировать, регуляризовать.
